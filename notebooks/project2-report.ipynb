{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "## MSDS 7349 - Section 401\n",
    "## Project 2\n",
    "\n",
    "[Data Science @ Southern Methodist University](https://datascience.smu.edu/)\n",
    "\n",
    "# Table of Contents\n",
    "* [Team Members](#Team-Members)\n",
    "* [Data Preparation](#Data-Preparation)\n",
    "* [Dataset Description](#Dataset-Description)\n",
    "* [Evaluation Metrics Description](#Evaluation-Metrics-Description)\n",
    "* [Training and Testing Splits method](#Training-Testing)\n",
    "* [Classification Task](#Classification-Task)\n",
    "* [Regression Task](#Regression-Task)\n",
    "* [Methods of Evaluation Results](#Methods-of-Evaluation)\n",
    "* [Advantages of each model](#Advantages-of-each-model)\n",
    "* [Relevant Attributes](#Relevant-Attributes)\n",
    "* [Deployment](#Deployment)\n",
    "* [Exceptional Work](#Exceptional-Work)\n",
    "* [References](#References)\n",
    "\n",
    "\n",
    "# <a name=\"Team-Members\"></a>Team Members\n",
    "* [Jostein Barry-Straume](https://github.com/josteinstraume)\n",
    "* [Kevin Cannon](https://github.com/kcannon2)\n",
    "* [Ernesto Carrera Ruvalcaba](https://github.com/ecarrerasmu)\n",
    "* [Adam Tschannen](https://github.com/adamtschannen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Data-Preparation\"></a>Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [10 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import statistics as st\n",
    "import csv as csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from __future__ import print_function\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "filepath = \"/data/credit-defaults.xls\"\n",
    "\n",
    "# Relative local path to avoid changing filepath constantly\n",
    "credit = pd.read_excel('../data/credit-defaults.xls', header=1, skiprows=0)\n",
    "\n",
    "\n",
    "# Rename column(s)\n",
    "credit = credit.rename(columns={'default payment next month': 'default_next_m', 'PAY_0': 'PAY_1'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING NEW VARIABLE TO IMPUTE THE VALUES TO 4, THAT REPRESETNS OTHER\n",
    "credit['EDUCATION_INP']=credit['EDUCATION']\n",
    "\n",
    "credit.loc[credit['EDUCATION'] > 4, 'EDUCATION_INP'] = 4\n",
    "credit.loc[credit['EDUCATION'] == 0, 'EDUCATION_INP'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "####FOR MARRIAGE\n",
    "#CREATING NEW VARIABLE TO IMPUTE THE VALUES TO 2, THAT REPRESETNS SINGLE, since the value of 0 is not defined in\n",
    "#the data dictionary\n",
    "credit['MARRIAGE_INP']=credit['MARRIAGE']\n",
    "\n",
    "\n",
    "credit.loc[credit['MARRIAGE'] == 0, 'MARRIAGE_INP'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### CREATING A FUNCTION TO GROUP VARIABLES\n",
    "def group_biv_v1(var_name,RangeName,CutOff,NumBins,BinLabel,var_x,var_y,title1,title2):\n",
    "#def bivariate_v1(NumBins): \n",
    "    #Creating the BINS\n",
    "    credit[RangeName]=pd.cut(var_name,CutOff,NumBins,labels=BinLabel)\n",
    "    #Grouping by the BINS variable\n",
    "    default_g=credit.groupby(by=[RangeName])\n",
    "    #% of POPULATION\n",
    "    bins_percentage=default_g[RangeName].count()/credit[RangeName].count()\n",
    "    \n",
    "    #### creating DUMMY VARIABLES\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#group_biv_v1(credit.PAY_1,'PAY_1_range',[-2.1,0,1,10],3,['Pay duly','delay 1 month ','delay > 1'],'Percentage','PAY_1_bins','% of population PAY_1','Default PAY_1 ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## IMPUTE VALUES FOR THE VARIABLES OF REPAYMENT STATUS (PAY0, PAY1,   PAYN)\n",
    "group_biv_v1(credit.PAY_1,'PAY_1_range',[-2.1,0,1,10],3,['Pay duly','delay 1 month ','delay > 1'],'Percentage','PAY_1_bins','% of population PAY_1','Default PAY_1 ')\n",
    "\n",
    "group_biv_v1(credit.PAY_2,'PAY_2_range',[-2.1,1,10],2,['Pay duly','delay > 1'],'Percentage','PAY_2_bins','% of population PAY_2','Default PAY_2 ')\n",
    "\n",
    "group_biv_v1(credit.PAY_3,'PAY_3_range',[-2.1,1,10],2,['Pay duly','delay > 1'],'Percentage','PAY_3_bins','% of population PAY_3','Default PAY_3 ')\n",
    "\n",
    "group_biv_v1(credit.PAY_4,'PAY_4_range',[-2.1,1,10],2,['Pay duly','delay > 1'],'Percentage','PAY_4_bins','% of population PAY_4','Default PAY_4 ')\n",
    "\n",
    "group_biv_v1(credit.PAY_5,'PAY_5_range',[-2.1,1,10],2,['Pay duly','delay > 1'],'Percentage','PAY_5_bins','% of population PAY_5','Default PAY_5 ')\n",
    "\n",
    "group_biv_v1(credit.PAY_6,'PAY_6_range',[-2.1,1,10],2,['Pay duly','delay > 1'],'Percentage','PAY_6_bins','% of population PAY_6','Default PAY_6 ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_1</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default_next_m</th>\n",
       "      <th>EDUCATION_INP</th>\n",
       "      <th>MARRIAGE_INP</th>\n",
       "      <th>PAY_1_range</th>\n",
       "      <th>PAY_2_range</th>\n",
       "      <th>PAY_3_range</th>\n",
       "      <th>PAY_4_range</th>\n",
       "      <th>PAY_5_range</th>\n",
       "      <th>PAY_6_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>delay &gt; 1</td>\n",
       "      <td>delay &gt; 1</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>delay &gt; 1</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>delay &gt; 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_1  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1      20000    2          2         1   24      2      2     -1     -1   \n",
       "1   2     120000    2          2         2   26     -1      2      0      0   \n",
       "2   3      90000    2          2         2   34      0      0      0      0   \n",
       "3   4      50000    2          2         1   37      0      0      0      0   \n",
       "4   5      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "      ...       PAY_AMT6  default_next_m  EDUCATION_INP  MARRIAGE_INP  \\\n",
       "0     ...              0               1              2             1   \n",
       "1     ...           2000               1              2             2   \n",
       "2     ...           5000               0              2             2   \n",
       "3     ...           1000               0              2             1   \n",
       "4     ...            679               0              2             1   \n",
       "\n",
       "   PAY_1_range  PAY_2_range  PAY_3_range  PAY_4_range  PAY_5_range  \\\n",
       "0    delay > 1    delay > 1     Pay duly     Pay duly     Pay duly   \n",
       "1     Pay duly    delay > 1     Pay duly     Pay duly     Pay duly   \n",
       "2     Pay duly     Pay duly     Pay duly     Pay duly     Pay duly   \n",
       "3     Pay duly     Pay duly     Pay duly     Pay duly     Pay duly   \n",
       "4     Pay duly     Pay duly     Pay duly     Pay duly     Pay duly   \n",
       "\n",
       "   PAY_6_range  \n",
       "0     Pay duly  \n",
       "1    delay > 1  \n",
       "2     Pay duly  \n",
       "3     Pay duly  \n",
       "4     Pay duly  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 33 columns):\n",
      "ID                30000 non-null int64\n",
      "LIMIT_BAL         30000 non-null int64\n",
      "SEX               30000 non-null int64\n",
      "EDUCATION         30000 non-null int64\n",
      "MARRIAGE          30000 non-null int64\n",
      "AGE               30000 non-null int64\n",
      "PAY_1             30000 non-null int64\n",
      "PAY_2             30000 non-null int64\n",
      "PAY_3             30000 non-null int64\n",
      "PAY_4             30000 non-null int64\n",
      "PAY_5             30000 non-null int64\n",
      "PAY_6             30000 non-null int64\n",
      "BILL_AMT1         30000 non-null int64\n",
      "BILL_AMT2         30000 non-null int64\n",
      "BILL_AMT3         30000 non-null int64\n",
      "BILL_AMT4         30000 non-null int64\n",
      "BILL_AMT5         30000 non-null int64\n",
      "BILL_AMT6         30000 non-null int64\n",
      "PAY_AMT1          30000 non-null int64\n",
      "PAY_AMT2          30000 non-null int64\n",
      "PAY_AMT3          30000 non-null int64\n",
      "PAY_AMT4          30000 non-null int64\n",
      "PAY_AMT5          30000 non-null int64\n",
      "PAY_AMT6          30000 non-null int64\n",
      "default_next_m    30000 non-null int64\n",
      "EDUCATION_INP     30000 non-null int64\n",
      "MARRIAGE_INP      30000 non-null int64\n",
      "PAY_1_range       30000 non-null category\n",
      "PAY_2_range       30000 non-null category\n",
      "PAY_3_range       30000 non-null category\n",
      "PAY_4_range       30000 non-null category\n",
      "PAY_5_range       30000 non-null category\n",
      "PAY_6_range       30000 non-null category\n",
      "dtypes: category(6), int64(27)\n",
      "memory usage: 6.4 MB\n"
     ]
    }
   ],
   "source": [
    "credit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 37 columns):\n",
      "ID                            30000 non-null int64\n",
      "LIMIT_BAL                     30000 non-null int64\n",
      "SEX                           30000 non-null int64\n",
      "EDUCATION                     30000 non-null int64\n",
      "MARRIAGE                      30000 non-null int64\n",
      "AGE                           30000 non-null int64\n",
      "PAY_1                         30000 non-null int64\n",
      "PAY_2                         30000 non-null int64\n",
      "PAY_3                         30000 non-null int64\n",
      "PAY_4                         30000 non-null int64\n",
      "PAY_5                         30000 non-null int64\n",
      "PAY_6                         30000 non-null int64\n",
      "BILL_AMT1                     30000 non-null int64\n",
      "BILL_AMT2                     30000 non-null int64\n",
      "BILL_AMT3                     30000 non-null int64\n",
      "BILL_AMT4                     30000 non-null int64\n",
      "BILL_AMT5                     30000 non-null int64\n",
      "BILL_AMT6                     30000 non-null int64\n",
      "PAY_AMT1                      30000 non-null int64\n",
      "PAY_AMT2                      30000 non-null int64\n",
      "PAY_AMT3                      30000 non-null int64\n",
      "PAY_AMT4                      30000 non-null int64\n",
      "PAY_AMT5                      30000 non-null int64\n",
      "PAY_AMT6                      30000 non-null int64\n",
      "default_next_m                30000 non-null int64\n",
      "EDUCATION_INP_2               30000 non-null uint8\n",
      "EDUCATION_INP_3               30000 non-null uint8\n",
      "EDUCATION_INP_4               30000 non-null uint8\n",
      "MARRIAGE_INP_2                30000 non-null uint8\n",
      "MARRIAGE_INP_3                30000 non-null uint8\n",
      "PAY_1_range_delay 1 month     30000 non-null uint8\n",
      "PAY_1_range_delay > 1         30000 non-null uint8\n",
      "PAY_2_range_delay > 1         30000 non-null uint8\n",
      "PAY_3_range_delay > 1         30000 non-null uint8\n",
      "PAY_4_range_delay > 1         30000 non-null uint8\n",
      "PAY_5_range_delay > 1         30000 non-null uint8\n",
      "PAY_6_range_delay > 1         30000 non-null uint8\n",
      "dtypes: int64(25), uint8(12)\n",
      "memory usage: 6.1 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 33 columns):\n",
      "ID                30000 non-null int64\n",
      "LIMIT_BAL         30000 non-null int64\n",
      "SEX               30000 non-null int64\n",
      "EDUCATION         30000 non-null int64\n",
      "MARRIAGE          30000 non-null int64\n",
      "AGE               30000 non-null int64\n",
      "PAY_1             30000 non-null int64\n",
      "PAY_2             30000 non-null int64\n",
      "PAY_3             30000 non-null int64\n",
      "PAY_4             30000 non-null int64\n",
      "PAY_5             30000 non-null int64\n",
      "PAY_6             30000 non-null int64\n",
      "BILL_AMT1         30000 non-null int64\n",
      "BILL_AMT2         30000 non-null int64\n",
      "BILL_AMT3         30000 non-null int64\n",
      "BILL_AMT4         30000 non-null int64\n",
      "BILL_AMT5         30000 non-null int64\n",
      "BILL_AMT6         30000 non-null int64\n",
      "PAY_AMT1          30000 non-null int64\n",
      "PAY_AMT2          30000 non-null int64\n",
      "PAY_AMT3          30000 non-null int64\n",
      "PAY_AMT4          30000 non-null int64\n",
      "PAY_AMT5          30000 non-null int64\n",
      "PAY_AMT6          30000 non-null int64\n",
      "default_next_m    30000 non-null int64\n",
      "EDUCATION_INP     30000 non-null int64\n",
      "MARRIAGE_INP      30000 non-null int64\n",
      "PAY_1_range       30000 non-null category\n",
      "PAY_2_range       30000 non-null category\n",
      "PAY_3_range       30000 non-null category\n",
      "PAY_4_range       30000 non-null category\n",
      "PAY_5_range       30000 non-null category\n",
      "PAY_6_range       30000 non-null category\n",
      "dtypes: category(6), int64(27)\n",
      "memory usage: 6.4 MB\n"
     ]
    }
   ],
   "source": [
    "#### CREATE DUMMY VARIABLES\n",
    "#Convert Categorical fields to bool using get_dummies\n",
    "#Use drop_first=true to aviod multicolinierarity\n",
    "### The drop_first=true eliminates the first group of the dummy variable\n",
    "### According to the notes provided of the professor we do this to prevent multicollinearity\n",
    "#### I can see it in the context of regression and logistic regression. We may want to select the group to delet\n",
    "##### The method \"get_dummies\" also ELIMINATES THE ORIGINAL VARIABLE TO CREATE THE dummies\n",
    "\n",
    "#EC: Creating  another reference in MEMORY, so the changes in df will not be REFLECTED IN CREDIT\n",
    "df = credit.copy()\n",
    "\n",
    "\n",
    "\n",
    "df = pd.get_dummies(data=credit, \n",
    "                       columns=['EDUCATION_INP',\n",
    "                                'MARRIAGE_INP', \n",
    "                                'PAY_1_range',\n",
    "                                'PAY_2_range',\n",
    "                                'PAY_3_range',\n",
    "                                'PAY_4_range',\n",
    "                                'PAY_5_range',\n",
    "                                'PAY_6_range',], drop_first=True) #Try drop_first=true to aviod multicolinierarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "credit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 41 columns):\n",
      "ID                            30000 non-null int64\n",
      "LIMIT_BAL                     30000 non-null int64\n",
      "SEX                           30000 non-null int64\n",
      "EDUCATION                     30000 non-null int64\n",
      "MARRIAGE                      30000 non-null int64\n",
      "AGE                           30000 non-null int64\n",
      "PAY_1                         30000 non-null int64\n",
      "PAY_2                         30000 non-null int64\n",
      "PAY_3                         30000 non-null int64\n",
      "PAY_4                         30000 non-null int64\n",
      "PAY_5                         30000 non-null int64\n",
      "PAY_6                         30000 non-null int64\n",
      "BILL_AMT1                     30000 non-null int64\n",
      "BILL_AMT2                     30000 non-null int64\n",
      "BILL_AMT3                     30000 non-null int64\n",
      "BILL_AMT4                     30000 non-null int64\n",
      "BILL_AMT5                     30000 non-null int64\n",
      "BILL_AMT6                     30000 non-null int64\n",
      "PAY_AMT1                      30000 non-null int64\n",
      "PAY_AMT2                      30000 non-null int64\n",
      "PAY_AMT3                      30000 non-null int64\n",
      "PAY_AMT4                      30000 non-null int64\n",
      "PAY_AMT5                      30000 non-null int64\n",
      "PAY_AMT6                      30000 non-null int64\n",
      "default_next_m                30000 non-null int64\n",
      "EDUCATION_INP_2               30000 non-null uint8\n",
      "EDUCATION_INP_3               30000 non-null uint8\n",
      "EDUCATION_INP_4               30000 non-null uint8\n",
      "MARRIAGE_INP_2                30000 non-null uint8\n",
      "MARRIAGE_INP_3                30000 non-null uint8\n",
      "PAY_1_range_delay 1 month     30000 non-null uint8\n",
      "PAY_1_range_delay > 1         30000 non-null uint8\n",
      "PAY_2_range_delay > 1         30000 non-null uint8\n",
      "PAY_3_range_delay > 1         30000 non-null uint8\n",
      "PAY_4_range_delay > 1         30000 non-null uint8\n",
      "PAY_5_range_delay > 1         30000 non-null uint8\n",
      "PAY_6_range_delay > 1         30000 non-null uint8\n",
      "Avg_BILL                      30000 non-null float64\n",
      "Avg_PAY                       30000 non-null float64\n",
      "AvgBill_to_LIMIT_BAL          30000 non-null float64\n",
      "AvgPay_to_LIMIT_BAL           30000 non-null float64\n",
      "dtypes: float64(4), int64(25), uint8(12)\n",
      "memory usage: 7.0 MB\n"
     ]
    }
   ],
   "source": [
    "##### CREATING ADDITIONAL VARIABLES \n",
    "\n",
    "############## CREATING NEW VARIABLES\n",
    "\n",
    "#average of hte bill amount\n",
    "df['Avg_BILL']=(df['BILL_AMT1']+df['BILL_AMT2']+df['BILL_AMT3']+ df['BILL_AMT4']+df['BILL_AMT5']+df['BILL_AMT6']\n",
    "               )/float(6)\n",
    "\n",
    "\n",
    "#Average of the Payment amount\n",
    "\n",
    "df['Avg_PAY']=(df['PAY_AMT1']+df['PAY_AMT2']+df['PAY_AMT3']+df['PAY_AMT4']+df['PAY_AMT5']+df['PAY_AMT6']\n",
    "               )/float(6)\n",
    "\n",
    "#We are constructing the ratio with the average BILL AMOUNT TO LIMIT\n",
    "df['AvgBill_to_LIMIT_BAL']=(df['Avg_BILL']/df['LIMIT_BAL'])\n",
    "\n",
    "\n",
    "#We are constructing the ratio with the  average PAYMENT AMOUNT TO LIMIT\n",
    "df['AvgPay_to_LIMIT_BAL']=(df['Avg_PAY']/df['LIMIT_BAL'])\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing variables not needed in the analysis\n",
    ">We will conduct one classification task and one regression task. We will create 2 different objects since we will remove different variables\n",
    "\n",
    "> We first create the object for the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WE CREATE AN OBJECT FOR THE CLASSIFICATION TASK AND OTHER FOR THE REGRESSION TASK\n",
    "\n",
    "# CREATING THE OBJECT FOR THE CLASSIFICATION TASK\n",
    "class_t=df.copy()\n",
    "reg_t=df.copy()\n",
    "\n",
    "if 'default_next_m' in class_t:\n",
    "    y_c = class_t['default_next_m'].values\n",
    "    del class_t['default_next_m']\n",
    "    del class_t['ID']\n",
    "    del class_t['EDUCATION']\n",
    "    del class_t['MARRIAGE']\n",
    "    del class_t['PAY_1']\n",
    "    del class_t['PAY_2']\n",
    "    del class_t['PAY_3']\n",
    "    del class_t['PAY_4']\n",
    "    del class_t['PAY_5']\n",
    "    del class_t['PAY_6']\n",
    "    X_c = class_t.values\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then we create the object for the Regression Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WE CREATE AN OBJECT FOR THE CLASSIFICATION TASK AND OTHER FOR THE REGRESSION TASK\n",
    "\n",
    "# CREATING THE OBJECT FOR THE REGRESSION  TASK\n",
    "\n",
    "reg_t=df.copy()\n",
    "\n",
    "\n",
    "if 'default_next_m' in reg_t:\n",
    "    y_r = reg_t['default_next_m'].values\n",
    "    del reg_t['default_next_m']\n",
    "    del reg_t['ID']\n",
    "    del reg_t['EDUCATION']\n",
    "    del reg_t['MARRIAGE']\n",
    "    del reg_t['PAY_1']\n",
    "    del reg_t['PAY_2']\n",
    "    del reg_t['PAY_3']\n",
    "    del reg_t['PAY_4']\n",
    "    del reg_t['PAY_5']\n",
    "    del reg_t['PAY_6']\n",
    "    del reg_t['Avg_BILL']\n",
    "    del reg_t['LIMIT_BAL']\n",
    "    del reg_t['AvgPay_to_LIMIT_BAL']\n",
    "    del reg_t['BILL_AMT1']\n",
    "    del reg_t['BILL_AMT2']\n",
    "    del reg_t['BILL_AMT3']\n",
    "    del reg_t['BILL_AMT4']\n",
    "    del reg_t['BILL_AMT5']\n",
    "    del reg_t['BILL_AMT6']\n",
    "    X_r = reg_t.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Dataset-Description\"></a>Dataset Description\n",
    "\n",
    ">[5 points] Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description for the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description for the Regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Evaluation-Metrics-Description\"></a>Evaluation Metrics Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">[10 points] Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
    "precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Training-Testing\"></a>Training and Testing Splits method\n",
    "\n",
    ">[10 points] Choose the method you will use for dividing your data into training and testing splits \n",
    "(i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is \n",
    "appropriate or use more than one method as appropriate. For example, \n",
    "if you are using time series data then you should be using continuous training and testing sets across time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Classification-Task\"></a>Classification Task\n",
    "\n",
    "> [20 points] Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Regression-Task\"></a>Regression Task\n",
    "\n",
    ">[20 points] Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Methods-of-Evaluation\"></a>Methods of Evaluation Results\n",
    "\n",
    "> [10 points] Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Advantages-of-each-model\"></a>Advantages of each model\n",
    ">[10 points] Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the cours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Relevant-Attributes\"></a>Relevant Attributes\n",
    ">[10 points] Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Deployment\"></a>Deployment\n",
    ">[5 points] How useful is your model for interested parties (i.e., the companies or organizations\n",
    "that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Exceptional-Work\"></a>Exceptional Work\n",
    "> [10 points] You have free reign to provide additional analyses\n",
    "> One idea: grid search parameters in a parallelized fashion and visualize the\n",
    "performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Training-and-Testing-Split\"></a>Training and Testing Split\n",
    "\n",
    "## Stratified K-Folds Cross-Validation\n",
    "\n",
    "> There is a very apparent class imbalance within the dataset. In order to address this issue, stratified K-Folds cross-validation will be employed to preserve the percentage of samples for each class. In other words, this will prevent the model from simply guessing 'paid duly' for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedShuffleSplit(n_splits=3, random_state=1, test_size=0.2,\n",
      "            train_size=0.8)\n",
      "TRAIN: [10029 17540 20357 ..., 13713 29455 19840] TEST: [24380  2508 10431 ...,  1932 21845  7320]\n",
      "TRAIN: [22092 12043 19732 ...,  9586 11830 13420] TEST: [10358  2644  6721 ..., 11354 22402 18779]\n",
      "TRAIN: [20650 10967 12984 ...,  1273 16044 28263] TEST: [23855 28179 26087 ..., 11323 26267  1297]\n"
     ]
    }
   ],
   "source": [
    "#df = credit\n",
    "#EC: Creating  another reference in MEMORY, so the changes in df will not be REFLECTED IN CREDIT\n",
    "#df = credit.copy()\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "if 'default_next_m' in df:\n",
    "    y = df['default_next_m'].values\n",
    "    del df['default_next_m']\n",
    "    del df['ID']\n",
    "    del df['EDUCATION']\n",
    "    del df['MARRIAGE']\n",
    "    X = df.values\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = StratifiedShuffleSplit(n_splits = num_cv_iterations, \n",
    "                            test_size = 0.20, train_size = 0.80, random_state=1)\n",
    "cv_object.get_n_splits(X, y)\n",
    "print(cv_object)\n",
    "for train_index, test_index in cv_object.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of obs in train set: 24000\n",
      "# of obs in test set: 6000\n",
      "# of defaults in train set: 5309\n",
      "Proportion of defaults in train set: 0.221208333333\n",
      "# of defaults in test set: 1327\n",
      "Proportion of defaults in test set: 0.221166666667\n"
     ]
    }
   ],
   "source": [
    "print('# of obs in train set: ' + str(len(y_train)))\n",
    "print('# of obs in test set: ' + str(len(y_test)))\n",
    "\n",
    "#train_defaults = sum(y_train == 0)\n",
    "train_defaults = sum(y_train == 1)\n",
    "print('# of defaults in train set: ' + str(train_defaults))\n",
    "\n",
    "proportion_train_defaults = train_defaults / len(y_train)\n",
    "print('Proportion of defaults in train set: ' + str(proportion_train_defaults))\n",
    "\n",
    "#test_defaults = sum(y_test == 0)\n",
    "test_defaults = sum(y_test == 1)\n",
    "print('# of defaults in test set: ' + str(test_defaults))\n",
    "\n",
    "proportion_test_defaults = test_defaults / len(y_test)\n",
    "print('Proportion of defaults in test set: ' + str(proportion_test_defaults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of obs in train set: 24000\n",
      "# of obs in test set: 6000\n",
      "# of defaults in train set: 5309\n",
      "Proportion of defaults in train set: 0.221208333333\n",
      "# of defaults in test set: 1327\n",
      "Proportion of defaults in test set: 0.221166666667\n"
     ]
    }
   ],
   "source": [
    "print('# of obs in train set: ' + str(len(y_train)))\n",
    "print('# of obs in test set: ' + str(len(y_test)))\n",
    "\n",
    "#train_defaults = sum(y_train == 0)\n",
    "train_defaults = sum(y_train == 1)\n",
    "print('# of defaults in train set: ' + str(train_defaults))\n",
    "\n",
    "proportion_train_defaults = train_defaults / len(y_train)\n",
    "print('Proportion of defaults in train set: ' + str(proportion_train_defaults))\n",
    "\n",
    "#test_defaults = sum(y_test == 0)\n",
    "test_defaults = sum(y_test == 1)\n",
    "print('# of defaults in test set: ' + str(test_defaults))\n",
    "\n",
    "proportion_test_defaults = test_defaults / len(y_test)\n",
    "print('Proportion of defaults in test set: ' + str(proportion_test_defaults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 5 neighbors is 76.3166666667 %\n",
      "Accuracy of 6 neighbors is 77.6 %\n",
      "Accuracy of 7 neighbors is 77.1333333333 %\n",
      "Accuracy of 8 neighbors is 78.0666666667 %\n",
      "Accuracy of 9 neighbors is 77.6666666667 %\n",
      "Accuracy of 10 neighbors is 78.2333333333 %\n",
      "Accuracy of 11 neighbors is 77.8333333333 %\n",
      "Accuracy of 12 neighbors is 78.0333333333 %\n",
      "Accuracy of 13 neighbors is 78.05 %\n",
      "Accuracy of 14 neighbors is 78.0666666667 %\n",
      "\n",
      "The best accuracy is 0.782333 with 10 neighbor(s)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.html import widgets \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn import metrics as mt\n",
    "\n",
    "\n",
    "max_accs = 0\n",
    "max_k = 0\n",
    "\n",
    "for k in range(5,15):\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, weights='uniform', metric='cosine')\n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat = clf.predict(X_test)\n",
    "    acc = mt.accuracy_score(y_test,yhat)\n",
    "    if max(max_accs, acc) == acc:\n",
    "        max_accs = acc\n",
    "        max_k = k\n",
    "    print('Accuracy of', k, 'neighbors is', acc * 100, '%')\n",
    "\n",
    "print('\\nThe best accuracy is %f with %d neighbor(s)'%(max_accs,max_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of l1 metrics is 53.6 %\n",
      "The best distance metric is:  l1\n",
      "\n",
      "Accuracy of l2 metrics is 54.1666666667 %\n",
      "The best distance metric is:  l2\n",
      "\n",
      "Accuracy of cosine metrics is 55.35 %\n",
      "The best distance metric is:  cosine\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "\n",
    "for d in ['l1','l2','cosine']:\n",
    "    clf = NearestCentroid(metric=d)\n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat = clf.predict(X_test)\n",
    "    acc = mt.accuracy_score(y_test,yhat)\n",
    "    print('\\nAccuracy of', d, 'metrics is', acc * 100, '%')    \n",
    "    print('The best distance metric is: ', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_1',\n",
       "       'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
       "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
       "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
       "       'default_next_m', 'EDUCATION_INP', 'MARRIAGE_INP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%time\n",
    "credit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LIMIT_BAL', 'SEX', 'AGE', 'PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5',\n",
       "       'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4',\n",
       "       'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3',\n",
       "       'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'EDUCATION_INP', 'MARRIAGE_INP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Logistic-Regression\"></a>Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We first run a logistic regression scenario using l2 normal penalty, a cost of 1 and assigning the same weight to the classes in the data.\n",
    "\n",
    ">Given the fact that the data is unbalanced, we are presenting the metrics derived from the confusion matrix (e.g. Precision, Recall, f1-score). A higher value of the metric \"Recall\" indicates that the model is predicting most defaults customers correctly.\n",
    "\n",
    ">Additionally, it is measured the time to fit and train the model to determine the cost of execution.\n",
    "\n",
    ">The performance of this first configuration is very poor since the \"Recall\" value for the category 1 (defaulted customers) is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time it takes to fit and predict is 1.34087491035\n",
      "\n",
      "====Iteration 0  ====\n",
      "accuracy 0.7785\n",
      "confusion matrix\n",
      " [[4671    2]\n",
      " [1327    0]]\n",
      "\n",
      " *** CLASSIFICATION REPORT ****\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      1.00      0.88      4673\n",
      "          1       0.00      0.00      0.00      1327\n",
      "\n",
      "avg / total       0.61      0.78      0.68      6000\n",
      "\n",
      "The time it takes to fit and predict is 1.17583584785\n",
      "\n",
      "====Iteration 1  ====\n",
      "accuracy 0.778666666667\n",
      "confusion matrix\n",
      " [[4672    1]\n",
      " [1327    0]]\n",
      "\n",
      " *** CLASSIFICATION REPORT ****\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      1.00      0.88      4673\n",
      "          1       0.00      0.00      0.00      1327\n",
      "\n",
      "avg / total       0.61      0.78      0.68      6000\n",
      "\n",
      "The time it takes to fit and predict is 0.998303890228\n",
      "\n",
      "====Iteration 2  ====\n",
      "accuracy 0.778833333333\n",
      "confusion matrix\n",
      " [[4673    0]\n",
      " [1327    0]]\n",
      "\n",
      " *** CLASSIFICATION REPORT ****\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      1.00      0.88      4673\n",
      "          1       0.00      0.00      0.00      1327\n",
      "\n",
      "avg / total       0.61      0.78      0.68      6000\n",
      "\n",
      "The average time to fit and predict 3 logistic regressions with 80/20 training/test split is: 1.17167154948\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from time import time\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "times_rec=[]\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    \n",
    "    #we count the time in executing the logistic regression\n",
    "    t0 = time()\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "    \n",
    "    t1=time()\n",
    "    diff=np.array([t1-t0])\n",
    "    \n",
    "    print (\"The time it takes to fit and predict is \" + str(diff[0]) + \"\\n\")    \n",
    "    times_rec=np.append(times_rec,diff)\n",
    "    \n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "    ClassReport = mt.classification_report(y_test,y_hat)\n",
    "    print(ClassReport)\n",
    "    \n",
    "    \n",
    "print(\"The average time to fit and predict 3 logistic regressions with 80/20 training/test split is: \" + str(times_rec.mean()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Adjust-Parameters-LR\"></a>Adjust Parameters for Logistic Regression\n",
    "\n",
    "> Let's adjust some parameters to see if a more accurate model can be produced.\n",
    "\n",
    "> By default, the LogisticRegression function within the SciKit Library gives all classes a weight of one. The class_weight parameter will be changed to 'balanced' to see if the accuracy of the models can be improved.\n",
    "\n",
    "> The results that follow, show an improvement in the performance, the \"Recall\" value is higher than 0.5, which is deemed adequate for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time it takes to fit and predict is 1.32330513\n",
      "\n",
      "====Iteration 0  ====\n",
      "accuracy 0.676166666667\n",
      "confusion matrix\n",
      " [[3235 1438]\n",
      " [ 505  822]]\n",
      "\n",
      " *** CLASSIFICATION REPORT ****\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.69      0.77      4673\n",
      "          1       0.36      0.62      0.46      1327\n",
      "\n",
      "avg / total       0.75      0.68      0.70      6000\n",
      "\n",
      "The time it takes to fit and predict is 1.23069000244\n",
      "\n",
      "====Iteration 1  ====\n",
      "accuracy 0.646166666667\n",
      "confusion matrix\n",
      " [[3089 1584]\n",
      " [ 539  788]]\n",
      "\n",
      " *** CLASSIFICATION REPORT ****\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.66      0.74      4673\n",
      "          1       0.33      0.59      0.43      1327\n",
      "\n",
      "avg / total       0.74      0.65      0.67      6000\n",
      "\n",
      "The time it takes to fit and predict is 1.59818077087\n",
      "\n",
      "====Iteration 2  ====\n",
      "accuracy 0.6605\n",
      "confusion matrix\n",
      " [[3094 1579]\n",
      " [ 458  869]]\n",
      "\n",
      " *** CLASSIFICATION REPORT ****\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.66      0.75      4673\n",
      "          1       0.35      0.65      0.46      1327\n",
      "\n",
      "avg / total       0.76      0.66      0.69      6000\n",
      "\n",
      "The average time to fit and predict 3 logistic regressions with 80/20 training/test split is: 1.38405863444\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(penalty='l2', C=1, class_weight='balanced') # get object\n",
    "#WARNING: THE FIRST WIEGHT WAS 1 \n",
    "iter_num=0\n",
    "times_rec=[]\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "     #we count the time in executing the logistic regression\n",
    "    t0 = time()\n",
    "    \n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "    \n",
    "    t1=time()\n",
    "    diff=np.array([t1-t0])\n",
    "    \n",
    "    print (\"The time it takes to fit and predict is \" + str(diff[0]) + \"\\n\")    \n",
    "    times_rec=np.append(times_rec,diff)\n",
    "    \n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    \n",
    "    print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "    ClassReport = mt.classification_report(y_test,y_hat)\n",
    "    print(ClassReport)\n",
    "    \n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print(\"The average time to fit and predict 3 logistic regressions with 80/20 training/test split is: \" + str(times_rec.mean()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, we will change the normal penalty to be L1 and assigning the same weight to the categories by setting the parameter \"class_weight\" to \"None.\n",
    "\n",
    "> By default, the LogisticRegression function within the SciKit Learn library uses 'L2' as its 'penalty' parameter. A binary class L2 penalized logistic regression minimizes the following cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://scikit-learn.org/stable/_images/math/760c999ccbc78b72d2a91186ba55ce37f0d2cf37.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='http://scikit-learn.org/stable/_images/math/760c999ccbc78b72d2a91186ba55ce37f0d2cf37.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now the 'penalty' parameter will be changed to L1 in order to optimize the following problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://scikit-learn.org/stable/_images/math/6a0bcf21baaeb0c2b879ab74fe333c0aab0d6ae6.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='http://scikit-learn.org/stable/_images/math/6a0bcf21baaeb0c2b879ab74fe333c0aab0d6ae6.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time it takes to fit and predict is 0.551182031631\n",
      "\n",
      "====Iteration 0  ====\n",
      "accuracy 0.81\n",
      "confusion matrix\n",
      " [[4549  124]\n",
      " [1016  311]]\n",
      "\n",
      " *** CLASSIFICATION REPORT ****\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.97      0.89      4673\n",
      "          1       0.71      0.23      0.35      1327\n",
      "\n",
      "avg / total       0.79      0.81      0.77      6000\n",
      "\n",
      "The time it takes to fit and predict is 0.464088916779\n",
      "\n",
      "====Iteration 1  ====\n",
      "accuracy 0.8095\n",
      "confusion matrix\n",
      " [[4538  135]\n",
      " [1008  319]]\n",
      "\n",
      " *** CLASSIFICATION REPORT ****\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.97      0.89      4673\n",
      "          1       0.70      0.24      0.36      1327\n",
      "\n",
      "avg / total       0.79      0.81      0.77      6000\n",
      "\n",
      "The time it takes to fit and predict is 0.505691289902\n",
      "\n",
      "====Iteration 2  ====\n",
      "accuracy 0.813\n",
      "confusion matrix\n",
      " [[4560  113]\n",
      " [1009  318]]\n",
      "\n",
      " *** CLASSIFICATION REPORT ****\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.98      0.89      4673\n",
      "          1       0.74      0.24      0.36      1327\n",
      "\n",
      "avg / total       0.80      0.81      0.77      6000\n",
      "\n",
      "The average time to fit and predict 3 logistic regressions with 80/20 training/test split is: 0.506987412771\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(penalty='l1', C=1.0, class_weight=None) # get object\n",
    "\n",
    "iter_num=0\n",
    "times_rec=[]\n",
    "\n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    \n",
    "    #we count the time in executing the logistic regression\n",
    "    t0 = time()\n",
    "    \n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "    \n",
    "    t1=time()\n",
    "    diff=np.array([t1-t0])\n",
    "    \n",
    "    print (\"The time it takes to fit and predict is \" + str(diff[0]) + \"\\n\")    \n",
    "    times_rec=np.append(times_rec,diff)\n",
    "    \n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    \n",
    "    print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "    ClassReport = mt.classification_report(y_test,y_hat)\n",
    "    print(ClassReport)\n",
    "    iter_num+=1\n",
    "    \n",
    "print(\"The average time to fit and predict 3 logistic regressions with 80/20 training/test split is: \" + str(times_rec.mean()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The use of L1 penalty did not improve the performance of the model since the value of \"Recall\" metric decreased. \n",
    "\n",
    "> Therefore, the best scenario tested at this point is using L2 normal penalty and setting the class_weight parameter to be “balanced”. The latter uses the values of response variable to automatically adjust weights inversely proportional to class frequencies in the input data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <a name=\"Interpreting-Weights\"></a>Interpreting Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.81    0.8095  0.813 ]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b534c06fb7134c7aa18244a48eae4055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIMIT_BAL has weight of -7.37786905546e-07\n",
      "SEX has weight of -0.106053514341\n",
      "AGE has weight of 0.00724341162499\n",
      "PAY_1 has weight of 0.588672865548\n",
      "PAY_2 has weight of 0.054184270252\n",
      "PAY_3 has weight of 0.0846198620851\n",
      "PAY_4 has weight of 0.00918660855435\n",
      "PAY_5 has weight of 0.0359400274746\n",
      "PAY_6 has weight of 0.0290397644058\n",
      "BILL_AMT1 has weight of -5.51985062786e-06\n",
      "BILL_AMT2 has weight of 3.40682484221e-06\n",
      "BILL_AMT3 has weight of 5.18112102741e-07\n",
      "BILL_AMT4 has weight of 1.41428698271e-06\n",
      "BILL_AMT5 has weight of -1.46807849532e-06\n",
      "BILL_AMT6 has weight of 4.69803853235e-07\n",
      "PAY_AMT1 has weight of -1.73718026445e-05\n",
      "PAY_AMT2 has weight of -7.3575013735e-06\n",
      "PAY_AMT3 has weight of -3.68879305577e-06\n",
      "PAY_AMT4 has weight of -1.65893976909e-06\n",
      "PAY_AMT5 has weight of -2.72880711536e-06\n",
      "PAY_AMT6 has weight of -1.58658500646e-06\n",
      "EDUCATION_INP has weight of -0.0989491529664\n",
      "MARRIAGE_INP has weight of -0.163727659611\n"
     ]
    }
   ],
   "source": [
    "# and here is an even shorter way of getting the accuracies for each training and test set\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(lr_clf, X, y=y, cv=cv_object) # this also can help with parallelism\n",
    "print(accuracies)\n",
    "\n",
    "\n",
    "# here we can change some of the parameters interactively\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def lr_explor(cost):\n",
    "    lr_clf = LogisticRegression(penalty='l2', C=cost, class_weight='balanced') # get object\n",
    "    accuracies = cross_val_score(lr_clf,X,y=y,cv=cv_object) # this also can help with parallelism\n",
    "    print(accuracies)\n",
    "\n",
    "    #WARNING=cost=(0.001,5.0,0.05)\n",
    "wd.interact(lr_explor,cost=(0.001,5.0,0.05),__manual=True)\n",
    "\n",
    "\n",
    "# interpret the weights\n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = df.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These weight interpretations are not necessarily interpretable because of the values we had. Very large attribute values could just as easily be assigned a higher weight. Instead, let's normalize the feature values so that all the attributes are on the same dynamic range. Once we normalize the attributes, the weights should have magnitudes that reflect their predictive power in the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.69\n",
      "[[3284 1389]\n",
      " [ 471  856]]\n",
      "BILL_AMT1 has weight of -0.374649398564\n",
      "PAY_AMT1 has weight of -0.262814724805\n",
      "PAY_AMT2 has weight of -0.149981769741\n",
      "LIMIT_BAL has weight of -0.0982022503934\n",
      "MARRIAGE_INP has weight of -0.0819311080116\n",
      "BILL_AMT5 has weight of -0.0752249404331\n",
      "EDUCATION_INP has weight of -0.0666072026459\n",
      "SEX has weight of -0.0539986548809\n",
      "PAY_AMT3 has weight of -0.0514898412435\n",
      "PAY_AMT5 has weight of -0.0362627540843\n",
      "PAY_AMT4 has weight of -0.0253744962584\n",
      "PAY_AMT6 has weight of -0.0217869630015\n",
      "BILL_AMT6 has weight of 0.00284234524141\n",
      "PAY_4 has weight of 0.00459520490125\n",
      "PAY_6 has weight of 0.00598116441579\n",
      "PAY_5 has weight of 0.0337073337868\n",
      "BILL_AMT3 has weight of 0.0518153248625\n",
      "BILL_AMT4 has weight of 0.0689899151578\n",
      "AGE has weight of 0.0732561227721\n",
      "PAY_2 has weight of 0.0736470020761\n",
      "PAY_3 has weight of 0.0980693683837\n",
      "BILL_AMT2 has weight of 0.222891699246\n",
      "PAY_1 has weight of 0.584867500528\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1, class_weight='balanced') # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,df.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAFCCAYAAAAZoN0UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XtcVHX+P/DXwChiiA13KV0MMSxN\nZREVTNNYtzbzwZqX2jQvXdZWzUupaXnLr4pm+igv3y0Ev0rpkuUldWuTTJMZL6ioWZqCkpqgwohI\ngDDM+f3Bj1kG5gzDnHHG4fN6Ph4+HnPmnPd5fwR9z5nP+ZzPRyVJkgQiIhKKh6sbQEREzsfiT0Qk\nIBZ/IiIBsfgTEQmIxZ+ISEAs/kREAmLxJyISEIs/EZGAWPyJiATE4k9EJCC1qxtgzdWrV2X3BQQE\noKCgoNHntDfO3XIqiWXOppVTSSxzul/O0NBQm87BK38iIgE55Mr/xIkTWL9+PYxGI5588kkkJCTU\nO0an02HLli1QqVT4wx/+gMmTJzsiNRER2UFx8TcajUhOTsa7774Lf39/zJo1C9HR0XjwwQdNx+Tl\n5WH79u1YuHAhfHx8cOvWLaVpiYhIAcXdPtnZ2QgJCUFwcDDUajViY2ORmZlpdsx3332HP//5z/Dx\n8QEAtG7dWmlaIiJSQPGVv16vh7+/v2nb398f58+fNzum5sbtnDlzYDQaMWzYMHTr1k1paiIispPi\n4m9pLRiVSmW2bTQakZeXh3nz5kGv12Pu3Ln44IMPcN9995kdl56ejvT0dABAYmIiAgIC5BuuVlvd\n7+g4d8upJJY5m1ZOJbHM2bRymp1DUTSqr/QLCwtN24WFhdBoNGbH+Pn5oWPHjlCr1QgKCkJoaCjy\n8vLQoUMHs+Pi4+MRHx9v2rY2DOpeG151r+VUEsucTSunkljmdL+cThvqGR4ejry8PFy/fh0GgwE6\nnQ7R0dFmx8TExOD06dMAgOLiYuTl5SE4OFhpaiIispPiK39PT0+MGzcOixYtgtFoRP/+/dG2bVuk\npaUhPDwc0dHR6Nq1K06ePImpU6fCw8MDI0eORKtWrRzR/ntW1auDTa+v1dnnmfSVcxtDRFSHQ8b5\nR0VFISoqyuy9ESNGmF6rVCqMHj0ao0ePdkQ6IiJSiE/4EhEJiMWfiEhALP5ERAJi8SciEhCLPxGR\ngFj8iYgExOJPRCQgFn8iIgGx+BMRCYjFn4hIQCz+REQCYvEnIhIQiz8RkYBY/ImIBMTiT0QkIBZ/\nIiIBsfgTEQmIxZ+ISEAs/kREAmLxJyISEIs/EZGAWPyJiATE4k9EJCAWfyIiAbH4ExEJiMWfiEhA\nLP5ERAJi8SciEhCLPxGRgFj8iYgExOJPRCQgFn8iIgGx+BMRCcghxf/EiROYPHkyJk2ahO3bt8se\nd+jQIQwfPhw5OTmOSEtERHZSXPyNRiOSk5Mxe/ZsrFy5ElqtFleuXKl3XFlZGb7++mtEREQoTUlE\nRAopLv7Z2dkICQlBcHAw1Go1YmNjkZmZWe+4tLQ0DB48GM2aNVOakoiIFFIrPYFer4e/v79p29/f\nH+fPnzc75uLFiygoKMAf//hH7Ny5U/Zc6enpSE9PBwAkJiYiICBA9li1Wm11v6PjGht7zcq+xuR3\nVnuZs+nmVBLLnE0rp9k5FEUDkCSp3nsqlcr02mg0YsOGDfjHP/7R4Lni4+MRHx9v2i4oKJA9NiAg\nwOp+R8cpja2tMedwRXuZs2nlVBLLnO6XMzQ01KZzKC7+/v7+KCwsNG0XFhZCo9GYtsvLy3H58mUs\nWLAAAFBUVIRly5ZhxowZCA8PV5qeiIjsoLj4h4eHIy8vD9evX4efnx90Oh3eeOMN0/6WLVsiOTnZ\ntD1//nyMGjWKhZ+IyIUUF39PT0+MGzcOixYtgtFoRP/+/dG2bVukpaUhPDwc0dHRjmgnERE5kOLi\nDwBRUVGIiooye2/EiBEWj50/f74jUhIRkQJ8wpeISEAs/kREAmLxJyISEIs/EZGAWPyJiATE4k9E\nJCAWfyIiAbH4ExEJiMWfiEhALP5ERAJi8SciEhCLPxGRgFj8iYgExOJPRCQgFn8iIgGx+BMRCYjF\nn4hIQCz+REQCYvEnIhIQiz8RkYBY/ImIBMTiT0QkIBZ/IiIBsfgTEQmIxZ+ISEAs/kREAmLxJyIS\nEIs/EZGAWPyJiATE4k9EJCC1qxtA5ExVrw42vb5WZ59n0lfObQyRC/HKn4hIQA658j9x4gTWr18P\no9GIJ598EgkJCWb7d+3ahe+++w6enp7w9fXF66+/jsDAQEekJiIiOyi+8jcajUhOTsbs2bOxcuVK\naLVaXLlyxeyYsLAwJCYmYvny5ejVqxc+/fRTpWmJiEgBxcU/OzsbISEhCA4OhlqtRmxsLDIzM82O\n6dy5M7y8vAAAERER0Ov1StMSEZECirt99Ho9/P39Tdv+/v44f/687PF79+5Ft27dLO5LT09Heno6\nACAxMREBAQGy51Gr1Vb3OzqusbF1bybW1pj8zmqvKDld/Xtxxc9WSSxzNq2cZudQFA1AkqR676lU\nKovH/vDDD7hw4QLmz59vcX98fDzi4+NN2wUFBbJ5AwICrO53dJzS2Noacw5XtFeUnHU54/fiqr+n\nO7WXOZXFhoaG2nQOxd0+/v7+KCwsNG0XFhZCo9HUO+7UqVPYtm0bZsyYgWbNmilNS0RECigu/uHh\n4cjLy8P169dhMBig0+kQHR1tdszFixeRlJSEGTNmoHXr1kpTEhGRQoq7fTw9PTFu3DgsWrQIRqMR\n/fv3R9u2bZGWlobw8HBER0fj008/RXl5OVasWAGg+ivLzJkzFTeeiIjs45Bx/lFRUYiKijJ7b8SI\nEabXc+bMcUQaIiJyED7hS0QkIBZ/IiIBsfgTEQmIxZ+ISEAs/kREAmLxJyISEIs/EZGAWPyJiATE\n4k9EJCAWfyIiAbH4ExEJiMWfiEhALP5ERAJi8SciEhCLPxGRgBwynz/dG6peHWy2XXuxcs+kr5zb\nGCK6p/HKn4hIQCz+REQCYvEnIhIQiz8RkYB4w9cKV9xAtZbzbuYlIrGw+BPdZbU/0PlhTvcKdvsQ\nEQmIxZ+ISEDs9iGyAR+go6aGV/5ERAJi8SciEhCLPxGRgNjnT4qwL5zIPfHKn4hIQCz+REQCYvEn\nIhKQQ/r8T5w4gfXr18NoNOLJJ59EQkKC2f7KykqsXr0aFy5cQKtWrTBlyhQEBQU5IjUREdlB8ZW/\n0WhEcnIyZs+ejZUrV0Kr1eLKlStmx+zduxf33XcfVq1ahWeeeQafffaZ0rRERKSA4iv/7OxshISE\nIDg4GAAQGxuLzMxMPPjgg6Zjjh49imHDhgEAevXqhZSUFEiSBJVKpTQ9OYCrZhLlhGdErqO4+Ov1\nevj7+5u2/f39cf78edljPD090bJlS9y+fRu+vr5mx6WnpyM9PR0AkJiYiICAANO+a3+NNTu2drEI\n3qaz2sbasXWLjNXYOvvUajUMBoPVXJZi7Y1zWqwrctaJbUyckn8Lrv69OOrvCdjwd61FrVab/X+6\nGzld8X+UOW3La4ni4i9JUr336l7R23IMAMTHxyM+Pt60XVBQYFMbbD1OaWxAQIBdueyNc1Wsu+Ws\nzRm/TyWxjvp7As77u9qb0xFxzNn42NDQUJtiFPf5+/v7o7Cw0LRdWFgIjUYje0xVVRVKS0vh4+Oj\nNDUREdlJcfEPDw9HXl4erl+/DoPBAJ1Oh+joaLNj/vjHP2Lfvn0AgEOHDuHRRx9lfz8RkQsp7vbx\n9PTEuHHjsGjRIhiNRvTv3x9t27ZFWloawsPDER0djQEDBmD16tWYNGkSfHx8MGXKFEe0nYiI7OSQ\ncf5RUVGIiooye2/EiBGm182bN8e0adMckYqIyKTuqDBH3lO5l3LeDXzCl4hIQCz+REQC4pTORCSk\n2t037tp1owSv/ImIBMTiT0QkICG6fUT/ekd0r+P/UefjlT8RkYBY/ImIBCREtw8R0b3gXure4pU/\nEZGAeOVPdI9qKtMI0L2JxZ+I6B53Ny4E2O1DRCQgFn8iIgG5TbcP+z+JiByHV/5ERAJi8SciEhCL\nPxGRgFj8iYgExOJPRCQgFn8iIgGx+BMRCYjFn4hIQCz+REQCYvEnIhKQ20zvQET3Nk7B4l545U9E\nJCAWfyIiAbH4ExEJiMWfiEhALP5ERAJi8SciEpCioZ4lJSVYuXIlbty4gcDAQEydOhU+Pj5mx+Tm\n5iIpKQllZWXw8PDAkCFDEBsbq6jRRESkjKLiv337dnTp0gUJCQnYvn07tm/fjpEjR5od07x5c0yc\nOBFt2rSBXq/H22+/ja5du+K+++5T1HAiIrKfom6fzMxM9OvXDwDQr18/ZGZm1jsmNDQUbdq0AQD4\n+fmhdevWKC4uVpKWiIgUUnTlf+vWLWg0GgCARqNpsKhnZ2fDYDAgODjY4v709HSkp6cDABITExEQ\nECB7LrVabXW/o+PcLaeS2Hs95zUr+xqT+17/e7oi1trPFnDOz1eU34ur/i2YztHQAQsXLkRRUVG9\n959//vlGJbp58yZWrVqFCRMmwMPD8heO+Ph4xMfHm7atPRpu76PjSh45d6ecSmLdLWdtjTmHu/09\nXRVbmzN+vqL8Xu5WztDQUJvO0WDxnzNnjuy+1q1b4+bNm9BoNLh58yZ8fX0tHldaWorExEQ8//zz\n6Nixo00NIyKiu0dRn390dDT2798PANi/fz969OhR7xiDwYDly5ejb9++6N27t5J0RETkIIr6/BMS\nErBy5Urs3bsXAQEBmDZtGgAgJycHe/bswfjx46HT6XDmzBncvn0b+/btAwBMmDABYWFhSttORER2\nUlT8W7Vqhblz59Z7Pzw8HOHh4QCAvn37om/fvkrSEBGRg/EJXyIiAbH4ExEJiMWfiEhALP5ERAJi\n8SciEhCLPxGRgFj8iYgEpGicPxE1LZ5JX5ltO2pOILr38MqfiEhALP5ERAJi8SciEhCLPxGRgFj8\niYgExOJPRCQgFn8iIgGx+BMRCYjFn4hIQCz+REQCYvEnIhIQiz8RkYBY/ImIBMTiT0QkIBZ/IiIB\nsfgTEQmIxZ+ISEAs/kREAmLxJyISEIs/EZGAWPyJiATE4k9EJCC1qxtA1FieSV+ZbQcEBKCgoMBF\nrSFyT7zyJyISkKIr/5KSEqxcuRI3btxAYGAgpk6dCh8fH4vHlpaWYurUqYiJicHLL7+sJC0RESmk\n6Mp/+/bt6NKlCz766CN06dIF27dvlz02LS0NjzzyiJJ0RETkIIqKf2ZmJvr16wcA6NevHzIzMy0e\nd+HCBdy6dQtdu3ZVko6IiBxEUfG/desWNBoNAECj0aC4uLjeMUajERs3bsTIkSOVpCIiIgdqsM9/\n4cKFKCoqqvf+888/b1OCb7/9Ft27d0dAQECDx6anpyM9PR0AkJiYaDVGrVbbdE5HxblbTiWxzNm0\nciqJZc6mldPsHA0dMGfOHNl9rVu3xs2bN6HRaHDz5k34+vrWO+bcuXM4c+YMvv32W5SXl8NgMKBF\nixZ48cUX6x0bHx+P+Ph407a14Xv2Du9TMizQnXIqiWXOppVTSSxzul/O0NBQm86haLRPdHQ09u/f\nj4SEBOzfvx89evSod8wbb7xher1v3z7k5ORYLPxEROQ8ivr8ExIScOrUKbzxxhs4deoUEhISAAA5\nOTn45z//6ZAGEhGR46kkSZJc3QgiInIut33C9+2333ZqnLvlVBLLnE0rp5JY5mxaOWtz2+JPRET2\nY/EnIhKQ5/z58+e7uhH2euihh5wa5245lcQyZ9PKqSSWOZtWzhq84UtEJCB2+xARCYjFn4hIQCz+\nREQCcvvir9frXd0EIiK34/bF/5133nH4OX/77TfT68rKSrN9586dU3TuU6dOWd1fWlqK/Pz8eu//\n+uuvDZ67qKjINANrcXExDh8+jMuXL9vVzk2bNjU65vr16zh8+LDZz09OQUEBKioqAACSJOH7779H\nSkoKvv32W1RVVTU6NxFQPc28s92+fdvmY4uLi5GTk4Pff//9LrbINm491BMAdu/ejUGDBlncl5KS\ngsjISDRr1szs/d9++w0ffvihaSGauhYtWoQ//elPAIDZs2ebXgPAsmXLzLYba/78+bLt1el0WLp0\nKY4fP45vvvkG4eHh8PPzAwAsWbLEat49e/Zg7dq12Lt3L5o1a4ZPP/0Uv//+O3bu3AkvLy+0b99e\nNjYlJQVZWVlmf/bu3Qu9Xo+srCx0797dYtyyZcsQFxcHoHphnw8//BAGgwFfffUVvL29ERYWJptz\nzpw5ePLJJ6FWq/HZZ58hOzsbXbp0wdmzZ3Hs2DGLkwQ25OOPP0Z0dLTsfqPRiPT0dGRmZtabEvfL\nL7+UXWnuzp072L17N86dO4f27dvjwIEDSEtLQ25uLjp27Ai1unHzI06ePBlPP/201WN+/fVX3H//\n/QAAg8GAbdu2YefOnbh06VKDOb/55hsEBQXBy8sL+fn5eP/997FhwwYcOXIEERERaN26daPaW2Px\n4sV4/PHHLe4rLS3Fli1bsG/fPlRWVqJdu3amfevWrUNUVJTseYuKipCamoqsrCxERERgx44dSElJ\nwU8//YTIyEh4e3tbjCspKUFFRYXpz507dzBr1iz069cPFRUVaN68uWzOEydOICQkxNT2devWIS0t\nDWfOnEFkZCRatGhhMe6zzz5DWFgYvLy8kJOTg7lz52Lv3r3YtWsX2rdvj8DAQNmc3333HT744AOc\nP38eX375JUJDQ22agfP8+fNITExEamoqjh07hocfftjiDMqNpWhWz3vd/fffjxkzZmDEiBHo06cP\n7ty5gy1btiAzM9PqzKK1R7/WHQlry8jYpUuXyp63pKRENm7btm1ITEyERqNBdnY2Vq9ejRdeeAE9\ne/ZsMO8333yDFStWoKKiAv/4xz+watUq3H///SgpKcGCBQswYMAA2dgjR47gkUceQdeuXU15tFpt\ng+OIa08pu2PHDsybNw9BQUEoLi7GwoUL8cQTT8jGGo1GeHl5AQB+/PFHLFmyBB4eHujbty+mT58u\nGyf385MkCVlZWVbb+8knn+DOnTvo0KED1q9fj0ceeQSjR48GUP0zeO655yzGrVmzBgEBAaioqEBi\nYiIeeOABPPvsszh27BiSkpIwadIk2ZwvvfQSVCqVqY1A9YdJzfsbNmywGLd27VrTv6NNmzbh9u3b\nePbZZ3HkyBEkJSVh4sSJsjm//fZbPPXUUwCA9evX45lnnkFMTAx++uknJCUlYeHChbKxFy5ckN2X\nm5sru2/t2rVo06YNevbsie+//x6HDh3C5MmT0axZM5w/f142Dqj++UZFReHOnTtYsGAB+vTpg1mz\nZiEzMxNJSUmYMWOGxbiXX3653pz2er0eM2fOhEqlwurVq2Vzbt68Gd26dQMAbNy4ERqNBjNnzsTh\nw4fxySefyOY8fvy4qXZ8+umnmDJlCjp06ICrV6/io48+QmJiomzOf//731ixYgV8fX1x7do1fPTR\nR1YvVmokJydj1KhR6NSpE44ePYoNGzY4pMfDLYp/SkqK7L7S0lLZfUOGDEGfPn2QnJyMPXv2QK/X\no3fv3li2bJmp8FhS85+17mtL25acPXsWkyZNqnf1IEkScnJyZOOMRqNpZbQOHTpg3rx5SExMRGFh\nYYN51Wo1vLy84OXlhZCQENNVo4+PT4OxK1euRFpaGk6cOIFRo0bBz88PX3zxhdXiDZj/LKqqqhAU\nFAQA8PX1bTBnQEAATp8+jc6dOyMwMBCFhYUIDAxs8Cv0yy+/jMDAQLMPQ5VKBUmSGvzKn52djeXL\nlwMAnnrqKaxbtw7Lly/H5MmTrX645uXlYdq0aZAkCa+99hrmzJkDlUqFTp06Wf2gAoAnnngCpaWl\nGDlypOl3MmHCBKxZs8ZqXO321Hw4qtVqm3LW7jYrLi5GTEwMAODRRx9FWVmZ1dhZs2bJfgOy1lVx\n7do1vPXWWwCAmJgYbN26Fe+9955sEa3t1q1bpm9C//nPf0yzAz/99NPYu3evbNyLL76IH3/8EaNG\njTJ907DlZ1tXTk4O3n//fQDAoEGDsH//ftljq6qqUFVVBU9PT1RUVKBDhw4AqufQr9tFXJdarTZd\nsQcHB8NgMNjUPkmS8NhjjwEAevfubXWt9MZwi+Jv7QrU1qfcqqqqIEkSHnzwQauFHwAKCwtNHzi1\nXwO23WCOiIhA8+bNLf4nsvY1z9vbG/n5+aavoxqNBvPnz8f777/fYN+9SqWCwWCAWq02m/SpoqKi\nwW8N3t7eGDNmDC5cuIBVq1ahe/fuNn3Dyc3NxejRoyFJEiorK1FUVIT7778fBoMBRqPRauzf//53\nrFmzBlu2bIG3tzdmzJiBsLAw/P7773jppZdk44KDgzF37lyLqxi9/vrrVnPW/s/m6emJv//97/ji\niy/w3nvvoby8vIG/bfXPuHv37qYPNpVK1eCH3Lhx43DhwgV8+OGH6NGjB5566imbLiBKS0tx5MgR\nGI1G0+/V1py9evXCmjVrMHToUPTo0QO7d+9Gz5498eOPPza4+tODDz6I1157DW3atKm3z9rPt+Z3\n7uFRfRtxyJAh8PPzw7x58xr82db+t1a3K9bav8PBgwcjLi4OGzZsgL+/P4YPH27Tzxao/sDZtWsX\nJElCWVkZJEmq9w3Nkj//+c9YsmQJEhIS0LVrV/zf//0fYmJicPr0aavdnED9WlJ3e9y4cRbjfv/9\ndxw+fFh2u2fPnlbzynGLPv+wsDCLf0JDQ3H9+nW0bdvWYtyXX36JjRs3YtCgQRg9ejR69eqF3bt3\n4z//+Q8iIiJk+818fX2h0Wig0Wjw2GOPmV7XbDf0S3788cdNV8F19e/fXzbuoYceglqtho+Pj+m9\nZs2aIS4uDiEhIfjDH/4gG9utWzd4e3vDw8MDLVu2NL1/69YttG/fHsHBwVbbDFR/2PTr1w+5ublQ\nqVQN/qMaNmwY/vrXv+Kvf/0rhg4davqmU1ZWho4dO8Lf3182tmXLlnjiiScQERGBwMBAREVFoWfP\nnhg+fLjpw88SlUoFX19f01V0bZ6enoiIiJCNPXfunOmbUY1HHnkEkiQhIyMDQ4cOtRh34cIFdO7c\nGWq12uxeRH5+PrKysqx2qQHVP9e+ffsiOzsbn376KYqLi/Hss89ajcnNzcXVq1eRl5cHf39/RERE\noEWLFigqKsKJEydk71cBQJcuXVBaWopNmzYhKysLp06dwtGjR+Hr64vRo0fXuwdWW+vWrdG6dWu0\natWq3r6goCA88MADFuMKCgqgUqnM/p2FhYUhODgYJ0+etHqP4+bNm+jQoQPUajU6d+5sej8/Px8X\nL15EbGysbGzLli3Ru3dvGAwGfPLJJyguLsbgwYNlj69RVlYGg8EAg8GAsLAwtG3bFl5eXigqKsKl\nS5dM35bqioiIgL+/P/bs2YNz584hPz8fly9fRocOHTBs2DDTh58lteuKpdoiV1dyc3ORl5dn+hMY\nGGi2bc/9MQCA5Gaqqqqk48ePS6tWrZJeeeUVafny5bLHpqSkSKWlpfXeP378uDRlypS72UybzJ49\n26lxrop1ZM5ffvnF7nPZG2stzmg0NipWr9dLx44dU5TzbsYeOnTIqXGOzHnnzh3p119/dWpOd+YW\n3T4A8PPPPyMjIwNZWVkIDw/HL7/8gtWrV1vtwhk7dqzF97t37252hVHX2bNnce3aNdPV1QcffGC6\n0fjcc89ZjW2MhvoIHR3nqlhH5kxOTpa9od4Qe2OtxTXUzVA3tuYqT0nOuxm7detWu7oR7I1zZM7m\nzZubjTJyRs7GWLt2rew+lUol261m7R6ESqVC37597WqPWxT/8ePHIyAgAAMHDsSoUaPg7e2NCRMm\nNNh3v2LFCkybNg1A9Z35kSNHmvYtXboU7777rsW4zz//3Kz/7erVq5gwYQLKy8uxbds2hxV/W/sn\nHRXnqlhXtZfoXmJpuGtBQQH+/e9/W71HZmmQiCRJOHbsGPR6fdMu/j179kRmZiZ0Oh08PDwQHR1t\nU1Go/bDUjz/+aLavuLhYNq6srAwPPvigabtNmzamG8ubN29ubPPJga5du2b1qnbmzJkOjxUlJ1D9\nDEzNqJ3apP9/Q7RmtJSj4kTK2atXL9Pra9euYdu2bThz5gwSEhKs3jeqfSEqSRIOHDiAHTt2ICIi\nAkOGDJGNa4hbFP+xY8dizJgx+Omnn5CRkYHU1FSUlZVBp9MhKipK9oEMax8Q1vbVHdJW+5dd8wSt\nI0h2zqZtb5yrYh2Z09fXt8GbpXLsjRUlJ1B9U9fah4Oj40TKCQBXrlzB1q1bkZubi8GDB+PVV1+F\np6dng3FVVVXYt28fdu3ahQ4dOuDNN9+06QExa9yi+APVxbpz587o3LkzDAYDTpw4Aa1Wi+TkZCQn\nJ1uMuXPnDi5evAhJklBRUWH2AEvN1AKWhIaG4vjx4/W+ph07dsymH/i6devwt7/9zWzUjSV1H9Sx\nN85Vsa7I2aJFC9lx6A2xN1aUnED1WHRrT6k6Ok6knCtWrEBOTg6effZZjBkzBh4eHmbPXdQe5Vfb\nN998g6+//hqdO3fG7Nmz7c5fl9sU/9rUajWio6MRHR1ttYhrNBps3LgRQPXTvqmpqaZ9loYK1hgz\nZgyWLFmCQ4cOmaZFuHDhAs6dO2fTp35gYCDefvttDB8+HH369JE9ru7NKXvjXBXripxyQ2htYW+s\nKDkB4OGHH3ZqnEg5a/rud+7caXrGoIa1J5LXr18PX19fnD171qw7z5auJmvcYiWvvLw8bN26FT4+\nPhg0aBA+/vhjnDlzBiEhIRg/fjzCw8MtxmVnZ8Pf3980umLfvn04fPgwAgMDMXz4cNlPWqB6lMmB\nAwdw5coVAEDbtm3Rpk0bZGRk4JVXXmmwzXq9Hhs2bMDt27cxcOBAs24ma6MF7I1zVayzc9Z+uMUS\nazntjRUlJwDs2rXLaqzcvFT2xomU0143btywut/ubz92RTnZ2rVr0a9fP5SWlmL27NkYM2YM3nrr\nLZw9exbJyclYvHixxbikpCTMmTMHQPVQ0c2bN2Ps2LHIzc3Fxx9/jDfffFM2Z7NmzTBgwABcvHgR\nWq0WX3zxBYKCgmwe5uXn54dY167BAAASb0lEQVSoqCj861//wtGjR80e/rB2DnvjXBXr7JwrVqxA\nWFiY7ANv1nLaGytKTgBITU1FWFgYunXrhmbNmtl8v8beOJFyWps3CZCfrcBR3Tx1ucWV//Tp001z\nb0yaNAmrVq2yuM9a3Lp16+Dr64vhw4c3GHf16lXodDpotVr4+PggNjYWO3futDpOt7bLly9j3bp1\n0Gg0GD16tE3jupXEuSrWFTmPHDkCnU6H/Px8REdHo0+fPlafCHZErCg5AeDixYvQ6XQ4ceIEHnro\nIcTFxaFLly4Njq6zN06knAsWLLC6f968eRbfrz05YG013T5ykwM2yDnPkikzY8YMi68tbdc2bdo0\nyWAwSJIkSZMnT5Z++ukns31yhg8fLs2dO1fKy8szvTdhwgSb2ztlyhTpxIkTNh+vNM5Vsa5qryRJ\nUllZmXTgwAFp6dKl0rvvvmv2u71bsaLkrHH27FkpOTlZmjJlipSZmXnX40TKac3Jkyftirt9+3aj\njneLbp+asbWSJJnNHihJEq5fvy4bFxcXh/nz56NVq1Zo3rw5OnXqBKB6/L+1USZvvvkmtFotFixY\ngK5duyIuLq5RX++WLVtmcf6Us2fPWr1nYG+cq2Jd1V6g+mnOli1bwtvb22xhGFvYGytKTqD6OZiL\nFy/i0qVL8PPzs3n+eHvjRMrZkM8++8w0i2djLFy4sFFPdrtF8V+5cqVdcUOGDEHnzp1RVFSExx57\nzPTVyWg0yk79AFRPSRsTE4Py8nJkZmZi9+7duHXrFpKSkhATE4OuXbtazVu7qOXm5iIjIwMHDx5E\nUFCQ7IRRSuJcFeuKnKdPn4ZWqzUt/vKXv/xF9oa/o2JFyQkA33//PXQ6HSorK9GrVy9MnTrVpsVf\n7I0TKaetGnOhqSTOLfr8bfXOO+9g0aJFd+XcJSUlOHjwIHQ6nWzfXA177xkoudfgilhX5BwxYgTa\ntWuHyMhIi/2gctPiKokVJWft2JoZWevGyw11tjdOpJy2mjlzpl1zMzU2zi2u/G2lZAKxhvj4+OBP\nf/qTTUs4Tp06FZGRkZg5c6bpRtvu3bvvWpyrYl2Rs6E5++9GrCg5AfmbjncrTqSc95omVfzvlUnA\n7L1noORegytiXZFTbnWxiooKHDt27K7EipITgOyTwQUFBdDpdLL77Y0TKaet7B3a2dhOnCZV/O8V\n9t4zUHKvwRWxrmpvDaPRiJMnT0Kr1eLkyZOIjIxE7969G4xTEitKTqD6puahQ4eg1Wqh1+ttXjTE\n3rimnvPnn3+2ur/mg6PuxHE1y50CwPXr182e4D58+LDpuY25c+fa1O4aTar432u3L1q0aIHHH38c\njz/+uOmewfbt2xssbPbGuSrW2TntWdtBaawoOcvKynDkyBFkZGQgLy8PMTExuHbtGv75z3/elTiR\ncn711Vf13lOpVPj1119RWFiItLQ0i3GpqammvvwPPvjArF+/9voC1mYssMQtiv///M//yM69X5ul\nCcRcoaKiAnv27EF+fj7atWuHAQMG2HTPwN44V8W6Iqe9azsoiRUlJwC88sor6NChA55//nnTDeMj\nR47ctTiRctZeWxuoHta8detWaDQaqzfha1/U1r3AVXLBK7/g5D3E2tz7tdm6is/dtmbNGuTk5KBd\nu3bIysoyTS53t+JcFeuKnD179oRer4dOp8OxY8dQXl5u870ee2NFyQkAL7zwAiorK7Fu3Tps27bN\nbE2MuxEnUs4aP/74I+bPn4+0tDQMGjQIixYtQnR0tOzxtX93dX+PihZKcoehnhMnTsSoUaNk99u7\nrNrd8uabb+KDDz4AUD0P9+zZs20agmVvnKtiXdVeSZJMaztkZWWhrKwM48ePt7q2g9JYUXLWuHbt\nGrRaLbRaLfLz8zFs2DDExMQ0OKW5vXEi5Dx+/Di2bt2Kli1bYsiQIYiMjGywfUD1LMOdOnWCJEk4\ne/as6WFVSZLwyy+/YP369Tadpy636PYpLS21OkrhXiv+avV/f6y2LNSgNM5Vsa5qrz1rOyiNFSVn\njeDgYAwZMgRDhgzBpUuXkJGRgSVLlpjNq+XIOBFyLl26FH5+fvDx8cGOHTuwY8cOs/1yzwjMmDHD\n9Hrw4MEN/p1s5RZX/vY+9OAqI0aMMF1dSf9/IRkvL68GJ2KyN85Vsa5qr5yKigo0b94cALB8+XKL\ny+05OlaUnHXZ+0Clkgcx3T2nraN9nMUtrvzd4PPJjNxd+7sV56pYV7VXTk1RA2B1zidHxoqSsy57\nH6hU8iCmu+esKe4VFRXIz8+HSqVCcHCw2e/EkszMTBQWFuKpp54CAMyePdt0H3TkyJFmawM3hlsU\n/0mTJrm6CeRmFN0IszNWlJxKYkXOWVVVhc2bN+P7779HQEAAJElCYWEh+vfvj+eff96sK7S2r776\nCpMnTzZtV1ZWYsmSJbhz5w7Wrl3btIv/O++8c3fmsyYicpLU1FSUl5dj9erV8Pb2BlB9PzM1NRWp\nqamyk00aDAYEBASYtiMjI9GqVSu0atUKd+7csbs9blH8GzOEkAhQ1lXorFkV3TWnkliRcx4/fhwf\nfvih2YVsy5Yt8eqrr2LKlCmyxb+kpMRs++WXXza9tnUYvCVuMc6/pKTE6h8iwHzq7xdffNEpsU0t\n57p161BaWtrgeeo+UGlvnEg5VSqVxR4MDw8Pq91LERERSE9Pr/f+nj17bJ6q2xK3uPJ/+eWX4efn\nZxoSWPtT1dqq9ySWc+fOmV7bMrWEI2KbWs7AwEC8/fbbGD58OPr06SN7nroPVNobJ1LOBx54APv3\n70e/fv3M3v/hhx+sPh8wevRovP/++9BqtWjfvj2A6vWAKysrMX36dNm4hrjFUM/169fj559/xsMP\nP4y4uDjZecpJbK+//jr+93//16mxTTGnXq/Hhg0bcPv2bQwcONDs/5q1Z2rsjRMlp16vx/Lly9G8\neXPTYu05OTmoqKjA9OnT4efnZ7W9p0+fxuXLlwEAbdu2NU32Zi+3uPIfO3as6YnFH374ASkpKeja\ntSsGDhxoNsMdNX0XLlyQ3VdVVXVXYkXJWcPPzw9RUVH417/+haNHj8LD47+9w9aKm71xouT08/PD\n4sWLTUVckiR0794dXbp0sdrOGjUP7QFAeXk5Dhw4gIyMDMyaNcum+LrcovgD/31isX379tBqtUhL\nS0NISAji4+Nd3TRyotTUVNl9DzzwwF2JFSUnAFy+fBnr1q2DRqPB4sWLodForB6vNE6knDX3J8PC\nwhAWFlbv/YZm5TQYDDh+/DgyMjJw8uRJ9OzZ06bFpeS4RbdPeXk5jh49Cp1Oh+LiYsTExCA2NtZs\n+BPR+fPnERER4dTYppZz6tSpGDNmTKPvJdgbJ1LOCRMmQKVSmYao16jZlrt3eerUKVPBf/TRRxEb\nG4v169djzZo1jW6DGckNjBw5Unrrrbekbdu2SQcPHpQOHTpk9odIkiRp/PjxTo9tajkrKiosvn/m\nzBkpKSnJ4XEi5bx+/brV/XKGDx8uzZ07V7p27ZrpvQkTJth1rtrcotunV69eUKlUuHr1Kq5evVpv\n/702sRuRu2rWrJnpdW5uLjIyMnDw4EEEBQUhJibG4XEi5Vy+fLldc5QlJiZCq9Vi4cKFCAoKQlxc\nHIxGY6PPU5dbFP8JEya4uglEQrh69Sp0Oh20Wi18fHwQGxsLSZIaXLjc3jiRckp29rC3b98e7du3\nx8iRI3H27FlotVoYDAYsXrwYMTExdt/3dIviv2vXLqv7Bw0a5KSWkKslJibKTvXR0AN/9saKkhOo\n7tOOjIzEzJkzERISAgDYvXu31RglcSLl1Ov1SElJkd1vbTWvGpGRkYiMjMTYsWNx6tQp6HS6pl38\ny8rKZPdxvL9YrM1n3tBc5/bGipITqF5kR6vVYsGCBejatSvi4uJsumK1N06knLXH9zeG3NBdX19f\n00yf9nCL0T7W7N69G88884yrm0HUpJSXlyMzMxNarRanT59Gv379EBMT0+AoF3vjRMhp77okCxYs\nsLrfli4nS9y++Ct52pHcz5tvvmn1297y5csdHitKTjklJSU4ePAgdDpdowqNvXFNNaeSRWXuBhZ/\ncis3btywuj8wMNDhsaLkBKoXGtmzZw/y8/PRrl07DBgwwKZlNu2NEymnJfn5+dBqtdDpdKY1rW11\n6tQp7NixA3PmzLErt1v0+RPVsFS4iouL0apVqwbv/9gbK0pOAFizZg08PT3RqVMnZGVl4cqVK7JT\nDTsiTqScNW7evAmdToeMjAxcunQJCQkJZou11HX69GkkJSVBr9ejR48eGDJkCFavXg1JkjBkyJBG\n5a7NLYr/Sy+9JDt6oaKiwgUtIlc5d+4cNm3aBB8fHzz33HNYvXo1iouLIUkSJk6ciG7dujk8VpSc\nAHDlyhXTFeiAAQMwe/Zs2WMdESdSzvT0dGi1Wuj1evTu3Rvjx4/HsmXLMGzYMKtxGzduxGuvvYaO\nHTsiKysL77zzDkaMGIG//OUvNue2xC2KPxdzoRopKSl44YUXUFpaivfeew+zZs1Cx44d8dtvv+HD\nDz+0WtjsjRUlJwCzpQQb051hb5xIOZOTk9GxY0e88cYbpnn4bRmtqFKp8OijjwIAYmJikJqaqrjw\nA25S/IlqVFVVmUZUfP755+jYsSOAhicsUxIrSk6g+qnV0aNHA/jvN+vRo0c3uGSqvXEi5fz4449x\n6NAhbNy4EUVFRejdu7dNs6z+/vvvOHz4sGlbkiSzbXtnOGDxJ7dSe/rc5s2bm+1r6CrK3lhRcgJA\nWlqa1f2OjhMpp6+vLwYOHIiBAweisLAQWq0Wvr6+mDp1Knr06IG//e1vFuMeeeQRHDt2THbb3uLv\n9qN9SCwjRoxAixYtTFddXl5eAKqvhiorK7F582aHx4qSk1zj6tWr0Gq1Dfb9OxqLPxGRE/zwww8A\ngL59+5q9n56ejhYtWlhdFtJoNKKkpAS+vr4Aquf237dvH3bv3m22LnNjuMUC7kRE7m7Xrl3o0aNH\nvffj4uKwc+dO2TitVouxY8di+vTpmDdvHk6fPo2JEyciKysLkyZNsrs97PMnInICo9EIb2/veu97\ne3tbvfG7detWLF26FCEhIbhw4QLeffddTJkypcEppBvCK38iIieoqqpCeXl5vffLyspgMBhk49Rq\ntWkG0YceesimtQNswSt/IiIn6N+/P1asWIFXXnkFQUFBAIDr168jOTkZAwYMkI27deuW2bT25eXl\nZtv2TmnPG75ERE7y7bffYvv27aZvAC1atEBCQgIGDhwoG7NlyxbZfSqVCkOHDrWrLSz+REROVl5e\nDkmSLN4DaIzs7Gx06NDBrlh2+xAROYGlFQl9fX0RGRlp6gayxZUrV6DVaqHVatGyZUskJiba1R4W\nfyIiJ7C0IuGNGzewdetWDBs2DHFxcbKxN27cMBV8Dw8PFBQUYMmSJY360KiLxZ+IyAnknuAtKSnB\nwoULZYv/u+++i9LSUsTGxmLatGlo06YNJkyYoKjwAxzqSUTkUj4+PlbXAfb19UVZWRlu3bqF4uJi\nAI5Zu5xX/kRELnT69Gncd999svtnzJiB0tJSHDp0CJ9//jny8/NRWlqq6GYvwNE+REROYWlt5ZKS\nEmg0GkycONGmKbcBoKioCDqdDjqdDoWFhXYvY8viT0TkBHXXVlapVPDx8UGLFi0UndPamszWsPgT\nETnRpUuX8NtvvwGoXmCnXbt2Vo9funSp1f0zZ860qx3s8ycicoLS0lIsW7YMhYWFaNeuHSRJwuXL\nlxEQEIDp06ejZcuWFuPOnTuHgIAAxMXFKerjr4tX/kRETpCSkgK1Wo2RI0eaVlszGo3YtGkTKioq\nMG7cOItxRqMRp06dQkZGBi5duoSoqCjExcWhbdu2itrD4k9E5ARTp07F8uXL6y38XlVVhbfeesum\nRVkqKyuh1WqRmpqKoUOH4umnn7a7Pez2ISJyArVaXa/wA4CnpyfUauuluLKyEsePH4dWq8WNGzfw\n9NNP2712r6k9iqKJiMgmlZWVuHjxosUHuqzN57969WpcvnwZ3bt3x9ChQxu8QWwrFn8iIie4//77\nsXHjRtl9cg4cOAAvLy/k5eXh66+/Nr0vSRJUKhU2bNhgV3vY509EJCDO7UNE5AQ7duwwvT548KDZ\nvk2bNjm7OSz+RETOoNPpTK+3b99utu/kyZPObg6LPxGRM9TuYa/b2+6K3ncWfyIiJ6g9qVvdCd4c\nMUVzY/GGLxGRE4wYMQItWrSAJEmoqKiAl5cXgOqr/srKSmzevNmp7WHxJyISELt9iIgExOJPRCQg\nFn8iIgGx+BMRCej/ARqWPm4wNON1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1a087f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=df.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bill amount correlation: (0.95148367275181633, 0.0)\n",
      "pay amount correlation: (0.28557552868684272, 0.0)\n"
     ]
    }
   ],
   "source": [
    "from pydoc import help\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "#correlation with in like variables \n",
    "bill = pearsonr(credit.BILL_AMT1, credit.BILL_AMT2)\n",
    "pay = pearsonr(credit.PAY_AMT1, credit.PAY_AMT2)\n",
    "\n",
    "\n",
    "print('bill amount correlation:', bill)\n",
    "print('pay amount correlation:', pay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> For more improvement and guarding against overfitting: At this point it make sense to remove variables highly related to one another and irrelevant ones and keep going with the weights analysis.\n",
    "\n",
    "> Initally only a handful of the explanatory variables seem to be worth keeping in the model. In particular, PAY_1, BILL_AMT1, BILL_AMT2, PAY_AMT1 and PAY_AMT2 have the greatest absolute weights. After taking a look at the correlations between BILL_AMT1 and BILL_AMT2 , with a correlation above .95, we can delete BILL_AMT2 to prevent over fitting. Like wise, we can keep both PAY_AMT1 and PAY_AMT2 becasue of the low correlation of .285.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age default correlation: (0.013889834301963198, 0.016136845890286453)\n",
      "marrige default correlation: (-0.02667216956912823, 3.8302915196419356e-06)\n",
      "gender default correlation: (-0.039960577705441529, 4.3952488033187112e-12)\n",
      "education default correlation: (0.0338422298256758, 4.5435584049859278e-09)\n"
     ]
    }
   ],
   "source": [
    "#correlation with response variable\n",
    "gender = pearsonr(credit.SEX, credit.default_next_m)\n",
    "Marrige = pearsonr(credit.MARRIAGE_INP, credit.default_next_m)\n",
    "age = pearsonr(credit.AGE, credit.default_next_m)\n",
    "education = pearsonr(credit.EDUCATION_INP, credit.default_next_m)\n",
    "#billD = pearsonr(credit.BILL_AMT1, credit.default_next_m)\n",
    "#payD1 = pearsonr(credit.PAY_AMT1, credit.default_next_m)\n",
    "#payD2 = pearsonr(credit.PAY_AMT2, credit.default_next_m)\n",
    "print('age default correlation:', age)\n",
    "print('marrige default correlation:', Marrige)\n",
    "print('gender default correlation:', gender)\n",
    "print('education default correlation:', education)\n",
    "#print('bill default correlation:', billD)\n",
    "#print('pay1 default correlation:', payD1)\n",
    "#print('pay2 default correlation:', payD2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Furthermore, Education, Marriage, Age and Sex appears to have only marginal significance in the model, but it could be argued to keep them in the model if all redundant PAY_X, BILL_AMTX, and PAY_AMTX variables are being removed. The argument to keep these variables in the model would be to help classify people before they have a payment or bill history with the company. For instance, a company could offer customer different products or marketing campaigns based on categories they would fall in if we used the attributes in our model.\n",
    "\n",
    "> After taking a further look at these \"profiling\" features, if seems reasonable to keep the top two features with the highest weights, Marriage and Education, in our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# you can apply the StandardScaler function inside of the cross-validation loop \n",
    "#  but this requires the use of PipeLines in scikit. \n",
    "#  A pipeline can apply feature pre-processing and data fitting in one compact notation\n",
    "#  Here is an example!\n",
    "\n",
    "std_scl = StandardScaler()\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1, class_weight='balanced') \n",
    "\n",
    "# create the pipline\n",
    "piped_object = Pipeline([('scale', std_scl),  # do this\n",
    "                         ('logit_model', lr_clf)]) # and then do this\n",
    "\n",
    "weights = []\n",
    "# run the pipline cross validated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    piped_object.fit(X[train_indices],y[train_indices])  # train object\n",
    "    # it is a little odd getting trained objects from a  pipeline:\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "\n",
    "weights = np.array(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from plotly import __version__\n",
    "#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "#plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "print(\"Plotly version: \" + plotly.__version__)           # version 1.9.x required\n",
    "plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': df.columns,\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = df[['PAY_1','BILL_AMT1','PAY_AMT1','PAY_AMT2','EDUCATION_INP','MARRIAGE_INP']].values\n",
    "\n",
    "weights = []\n",
    "# run the pipline corssvalidated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(Xnew,y)):\n",
    "    piped_object.fit(Xnew[train_indices],y[train_indices])  # train object\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "weights = np.array(weights)\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': ['PAY_1','BILL_AMT1','PAY_AMT1','PAY_AMT2','EDUCATION_INP','MARRIAGE_INP'],\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At first glance one might think each bill and payment period should have equal weight within the logistic regression model. However, clearly the first bill and payment period have the most significance upon viewing the above plots. Perhaps the greater weight of the first period compared to the other five can be explained by the recency of the period. \n",
    "\n",
    "> Simply put, if a customer fails to pay their credit bill in the first opportunity to pay, they are marked as having defaulted in the dataset. In other words, if a customer is likely to default on their bills, then the first chance for said customer is the first period.\n",
    "\n",
    "> If a customer has paid duly for periods 1-5, then they are likely to also pay their credit bill on the 6th go around, thus exhibiting they are fiscally sound as a customer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Support-Vector-Machines\"></a>Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "# Big Data, in all its glory:\n",
    "Image(url='http://flowingdata.com/wp-content/uploads/2014/11/Big-Data-620x465.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In this section, we will explore different configurations of SVM to derive a model to classify defaulted and non-defaulted customers.\n",
    "\n",
    "> As we did for the logistic regression, we are presenting the metrics derived from the confusion matrix (e.g. Precision, Recall, f1-score) since our data in unbalanced. A higher value of the metric \"Recall\" indicates that the model is predicting most defaults customers correctly. Lastly, the time to fit and predict is also measured.\n",
    "\n",
    ">The first configuration tested uses a \"linear\" kernel with a cost of 0.5 (C-parameter) a degree of 3 and gamma set to auto.\n",
    "\n",
    ">According to sklearn documentation:\n",
    "\n",
    ">Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.\n",
    "\n",
    ">The C parameter trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly by giving the model freedom to select more samples as support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, so run through the cross validation loop and set the training and testing variable for one single iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels - CHANGING THE KERNEL TO BE LINEAR\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "#warning\n",
    "#svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "svm_clf = SVC(C=0.5, kernel='linear', degree=3, gamma='auto') # get object\n",
    "\n",
    "#we count the time in executing the logistic regression\n",
    "t0 = time()\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "t1=time()\n",
    "diff=t1-t0\n",
    "    \n",
    "print (\"The time it takes to fit and predict is \" + str(diff) + \"\\n\")  \n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)\n",
    "\n",
    "\n",
    "print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "ClassReport = mt.classification_report(y_test,y_hat)\n",
    "print(ClassReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The value of recall exhibited above is of 0.23 which is considerably lower than the best scenario of the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, keeping other parameters equal, we use the Radial Basis Function (RBF) kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels - CHANGING THE KERNEL TO BE RBF\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "#warning\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "#svm_clf = SVC(C=0.5, kernel='linear', degree=3, gamma='auto') # get object\n",
    "\n",
    "#we count the time in executing the logistic regression\n",
    "t0 = time()\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "t1=time()\n",
    "diff=t1-t0\n",
    "    \n",
    "print (\"The time it takes to fit and predict is \" + str(diff) + \"\\n\")  \n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)\n",
    "\n",
    "\n",
    "print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "ClassReport = mt.classification_report(y_test,y_hat)\n",
    "print(ClassReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The results above show a higher value of the \"Recall\" metric which in this case indicates an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Following with the configuration presented above, now we change the parameter C equals to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels - CHANGING THE KERNEL TO BE RBF\n",
    "#AND CHANGING THE COST TO BE 2\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "#warning\n",
    "svm_clf = SVC(C=2, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "#svm_clf = SVC(C=0.5, kernel='linear', degree=3, gamma='auto') # get object\n",
    "\n",
    "#we count the time in executing the logistic regression\n",
    "t0 = time()\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "t1=time()\n",
    "diff=t1-t0\n",
    "    \n",
    "print (\"The time it takes to fit and predict is \" + str(diff) + \"\\n\")  \n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)\n",
    "\n",
    "\n",
    "print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "ClassReport = mt.classification_report(y_test,y_hat)\n",
    "print(ClassReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The results above show a higher value of the \"Recall\" metric which in this case indicates an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Using the latest configuration, now we change the parameter gamma to be 1e1. This change does not improve the performance of the model as the results below exhibit since the value of the recall decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels - CHANGING THE KERNEL TO BE RBF\n",
    "#AND CHANGING THE WEIGHT TO BE 2 AND GAMMA=1e1\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "#warning\n",
    "svm_clf = SVC(C=2, kernel='rbf', degree=3, gamma=1e1) # get object\n",
    "#svm_clf = SVC(C=0.5, kernel='linear', degree=3, gamma='auto') # get object\n",
    "\n",
    "#we count the time in executing the logistic regression\n",
    "t0 = time()\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "t1=time()\n",
    "diff=t1-t0\n",
    "    \n",
    "print (\"The time it takes to fit and predict is \" + str(diff) + \"\\n\")  \n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)\n",
    "\n",
    "\n",
    "print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "ClassReport = mt.classification_report(y_test,y_hat)\n",
    "print(ClassReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The best configuration identified at this point for SVM uses the \"rbf\" kernel with a cost of 2, degree of 3 and the gamma parameter equal to 2. To validate the performance of this configuration, to avoid overfitting we execute a cross-validation with 3 folds. The results in the \"recall\" metric are very similar, as exhibited below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### CV FOR SVM TO VALIDATE OVERFITTING #######################\n",
    "\n",
    "# we are using the best scenario to run the SVM AFTER changing some of the parameters\n",
    "#lr_clf = LogisticRegression(penalty='l1', C=1.0, class_weight=None) # get object\n",
    "svm_clf = SVC(C=2, kernel='rbf', degree=3, gamma='auto')\n",
    "\n",
    "\n",
    "iter_num=0\n",
    "times_rec=[]\n",
    "\n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    #### SCALING THE  FOLD\n",
    "    #OBTAINING THE TUNING PARAMETERS FOR EACHV ARIABLE IN THE TRAINING SAMPLE\n",
    "    scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "    # the line of code above only looks at training data to get mean and std and we can use it \n",
    "    # to transform new feature data\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "    X_test_scaled = scl_obj.transform(X_test)\n",
    "    \n",
    "    \n",
    "    #we count the time in executing the logistic regression\n",
    "    t0 = time()\n",
    "    \n",
    "    svm_clf.fit(X_train_scaled,y_train)  # train object\n",
    "    y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "    \n",
    "    t1=time()\n",
    "    diff=np.array([t1-t0])\n",
    "    \n",
    "    print (\"The time it takes to fit and predict is \" + str(diff[0]) + \"\\n\")    \n",
    "    times_rec=np.append(times_rec,diff)\n",
    "    \n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    \n",
    "    print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "    ClassReport = mt.classification_report(y_test,y_hat)\n",
    "    print(ClassReport)\n",
    "    iter_num+=1\n",
    "    \n",
    "print(\"The average time to fit and predict 3 SVM with 80/20 training/test split is: \" + str(times_rec.mean()) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Chosen-Support-Vectors-Analysis\"></a>Chosen Support Vectors Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this section, we take a look at the chosen support vectors (with the best scenario obtained in the prior section) for the classification task in order to identify any insight into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ FOR THE CONFIGURATION SELECTED WE NEED TO RE-RUN THE CODE TO DO THE ANALYSIS OF\n",
    "###### CHOSEN SUPPORT VECTORS\n",
    "\n",
    "# lets investigate SVMs on the data and play with the parameters and kernels - CHANGING THE KERNEL TO BE RBF\n",
    "#AND CHANGING THE COST TO BE 2\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "#warning\n",
    "svm_clf = SVC(C=2, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "#svm_clf = SVC(C=0.5, kernel='linear', degree=3, gamma='auto') # get object\n",
    "\n",
    "#we count the time in executing the logistic regression\n",
    "t0 = time()\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "t1=time()\n",
    "diff=t1-t0\n",
    "    \n",
    "print (\"The time it takes to fit and predict is \" + str(diff) + \"\\n\")  \n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)\n",
    "\n",
    "\n",
    "print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "ClassReport = mt.classification_report(y_test,y_hat)\n",
    "print(ClassReport)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do some different analysis with the SVM and look at the instances that were chosen as support vectors\n",
    "\n",
    "# now lets look at the support for the vectors and see if we they are indicative of anything\n",
    "# grabe the rows that were selected as support vectors (these are usually instances that are hard to classify)\n",
    "\n",
    "# make a dataframe of the training data\n",
    "df_tested_on = df.iloc[train_indices] # saved from above, the indices chosen for training\n",
    "# now get the support vectors from the trained model\n",
    "df_support = df_tested_on.iloc[svm_clf.support_,:]\n",
    "\n",
    "df_support['default_next_m'] = y[svm_clf.support_] # add back in the 'Survived' Column to the pandas dataframe\n",
    "df['default_next_m'] = y # also add it back in for the original data\n",
    "df_support.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following charts exhibit the KDE broke down by defaulted and non-defaulted customers. The charts on the left show the chosen support vectors and the charts on the right present the distribution of the variable. At the end, Support Vectors are simply the coordinates of individual observation. A Support Vector is a frontier which best segregates the two classes (hyper-plane/ line).\n",
    "\n",
    ">For example, the variable \"Pay_1\", the value of \"0\" shows a smaller gap between defaulted and non-defaulted, in other words, we are seeing the observations that are deemed as an error and used by the SVM to build the frontier.\n",
    "\n",
    ">The distributions for the variable \"Marriage_INP\" between the chosen support vectors and the real data looks very similar, that is telling us that there were not many instances identified as \"errors\" and therefore selected as Support Vectors based on the values for this variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets see the statistics of these attributes\n",
    "from pandas.tools.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby(['default_next_m'])\n",
    "df_grouped = df.groupby(['default_next_m'])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['PAY_1','BILL_AMT1','PAY_AMT1','EDUCATION_INP','MARRIAGE_INP']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['Default','Paid Duly'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['Default','Paid Duly'])\n",
    "    plt.title(v+' (Original)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By plotting the original data density statistics next to the statistics for the support vectors, we can look at the separation between the distributions of the defaults and those that paid duly. Generally, the separations between the original data will be larger than the separations from the support vector data, since the support vectors use points on the decision boundary to create the dividing line for classification. Otherwise, the points that are used are errors. Since the support vectors use edge points, the kernel density estimation lines should appear close together. And, the data for the PAY_1 and BILL_AMT1 variables do indeed have smaller separations between the distributions for the support vectors. This development is consistent with the weights calculated previously. Since PAY_1 and BILL_AMT1 are so much more significant to the model predictions than any of the other variables, it makes sense that these two variables make up many of the support vectors. Since more data points using those two variables are used from the original data from the support vectors, the distribution between two is a closer match, as seen in the charts.\n",
    "\n",
    "> However, the three remaining variables – PAY_AMT1, EDUCATION_INP, and MARRIAGE_INP - have similar or even slightly larger separations between the distributions. Since these variables are not weighted as heavily as the two primary prediction variables, the data diverges slightly between the original and support vector plots. The separation can vary depending on the size of the margins. These values could have error values that negatively impact the KDE lines that are drawn between the original and support vector data, leading to the slightly increased separations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint Plot\n",
    "# Source:\n",
    "# http://seaborn.pydata.org/generated/seaborn.jointplot.html\n",
    "import seaborn as sns\n",
    "\n",
    "# Original dataset\n",
    "g = sns.jointplot(\"MARRIAGE_INP\", \"PAY_1\", data=df, kind=\"kde\", space=0, color=\"g\")\n",
    "\n",
    "# Support Vector\n",
    "g = sns.jointplot(\"MARRIAGE_INP\", \"PAY_1\", data=df_support, kind=\"kde\", space=0, color=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Looking at the bivariate distribution of the two variables, the correlation between the PAY_1 variable and the MARRIAGE_INP improves in the support vector data over the original data. As seen in the plots, the density around all combinations of points improve and is most noticeable around the central points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint Plot\n",
    "# Source:\n",
    "# http://seaborn.pydata.org/generated/seaborn.jointplot.html\n",
    "import seaborn as sns;\n",
    "x = df_support['MARRIAGE_INP']\n",
    "y = df_support['PAY_1']\n",
    "g = (sns.jointplot(x, y, kind=\"hex\", stat_func=None).set_axis_labels(\"Marriage\", \"Pay_1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Taking another look at the PAY_1 and MARRIAGE_INP variables for the support vector data, the density improvement around the central values can be seen in the very dark points in the joint histogram using hexagonal bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(\"BILL_AMT1\", \"PAY_AMT1\", data=df, kind=\"hex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># <a name=\"Advantages-of-each-model\"></a>Advantages of each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For this classification problem, we recommend to use the logistic regression based upon the following:\n",
    "    \n",
    "       >1)The classification metrics of \"Recall\" and \"F1-score\" are higher for the logistic regression. The data is unbalanced and therefore, the \"Recall\" is a better metric to measure the performance of the model since a high value means that it is predicting more default customers correctly. \n",
    "       \n",
    "       >2) The time of execution is considerably higher for SVM. On average, the time taken to fit and predict the model is around 1 second using logistic regression whereas the SVM takes around 30 seconds. \n",
    "       \n",
    "       >3) The weights of the logistic regression provide a method to derive the importance of each variable. The bank running this model would be interested to know what variables are key to identify defaults to create segments that allow them to offer customer different products or marketing campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <a name=\"References\"></a>References\n",
    "\n",
    "* https://github.com/eclarson/DataMiningNotebooks/blob/master/04.%20Logits%20and%20SVM.ipynb\n",
    "* https://github.com/jakemdrew/EducationDataNC/blob/master/Graduation%20Rates%20v2.ipynb\n",
    "* http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "* https://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn\n",
    "* http://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/\n",
    "* https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
