{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "## MSDS 7349 - Section 401\n",
    "## Project 2\n",
    "\n",
    "[Data Science @ Southern Methodist University](https://datascience.smu.edu/)\n",
    "\n",
    "# Table of Contents\n",
    "* [Team Members](#Team-Members)\n",
    "* [Training and Testing Split](#Training-and-Testing-Split)\n",
    "* [Logistic Regression](#Logistic-Regression)\n",
    "* [Adjust Parameters for Logistic Regression](#Adjust-Parameters-LR)\n",
    "* [Interpreting Weights](#Interpreting-Weights)\n",
    "* [Support Vector Machines](#Support-Vector-Machines)\n",
    "* [Chosen Support Vectors Analysis](#Chosen-Support-Vectors-Analysis)\n",
    "* [Advantages of each model](#Advantages-of-each-model)\n",
    "* [References](#References)\n",
    "\n",
    "\n",
    "# <a name=\"Team-Members\"></a>Team Members\n",
    "* [Jostein Barry-Straume](https://github.com/josteinstraume)\n",
    "* [Kevin Cannon](https://github.com/kcannon2)\n",
    "* [Ernesto Carrera Ruvalcaba](https://github.com/ecarrerasmu)\n",
    "* [Adam Tschannen](https://github.com/adamtschannen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [50 points] Create a logistic regression model and a support vector machine model for the\n",
    "classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable time frame.\n",
    "\n",
    "> [10 points] Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail.\n",
    "\n",
    "> [30 points] Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?\n",
    "\n",
    "> [10 points] Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try sub-sampling your data to train the SVC model— then analyze the support vectors from the sub-sampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/credit-defaults.xls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c8b1300d77cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Relative local path to avoid changing filepath constantly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mcredit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/credit-defaults.xls'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheetname, header, skiprows, skip_footer, index_col, names, parse_cols, parse_dates, date_parser, na_values, thousands, convert_float, has_index_names, converters, dtype, true_values, false_values, engine, squeeze, **kwds)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     return io._parse_excel(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, **kwds)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/credit-defaults.xls'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import statistics as st\n",
    "import csv as csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from __future__ import print_function\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "filepath = \"/data/credit-defaults.xls\"\n",
    "\n",
    "# Relative local path to avoid changing filepath constantly\n",
    "credit = pd.read_excel('../data/credit-defaults.xls', header=1, skiprows=0)\n",
    "\n",
    "\n",
    "# Rename column(s)\n",
    "credit = credit.rename(columns={'default payment next month': 'default_next_m', 'PAY_0': 'PAY_1'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING NEW VARIABLE TO IMPUTE THE VALUES TO 4, THAT REPRESETNS OTHER\n",
    "credit['EDUCATION_INP']=credit['EDUCATION']\n",
    "\n",
    "credit.loc[credit['EDUCATION'] > 4, 'EDUCATION_INP'] = 4\n",
    "credit.loc[credit['EDUCATION'] == 0, 'EDUCATION_INP'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####FOR MARRIAGE\n",
    "#CREATING NEW VARIABLE TO IMPUTE THE VALUES TO 2, THAT REPRESETNS SINGLE, since the value of 0 is not defined in\n",
    "#the data dictionary\n",
    "credit['MARRIAGE_INP']=credit['MARRIAGE']\n",
    "\n",
    "\n",
    "credit.loc[credit['MARRIAGE'] == 0, 'MARRIAGE_INP'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Training-and-Testing-Split\"></a>Training and Testing Split\n",
    "\n",
    "## Stratified K-Folds Cross-Validation\n",
    "\n",
    "> There is a very apparent class imbalance within the dataset. In order to address this issue, stratified K-Folds cross-validation will be employed to preserve the percentage of samples for each class. In other words, this will prevent the model from simply guessing 'paid duly' for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = credit\n",
    "#EC: Creating a another reference in MEMORY, so the changes in df will not be REFLECTED IN CREDIT\n",
    "df = credit.copy()\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "if 'default_next_m' in df:\n",
    "    y = df['default_next_m'].values\n",
    "    del df['default_next_m']\n",
    "    del df['ID']\n",
    "    del df['EDUCATION']\n",
    "    del df['MARRIAGE']\n",
    "    X = df.values\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = StratifiedShuffleSplit(n_splits = num_cv_iterations, \n",
    "                            test_size = 0.20, train_size = 0.80, random_state=1)\n",
    "cv_object.get_n_splits(X, y)\n",
    "print(cv_object)\n",
    "for train_index, test_index in cv_object.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# of obs in train set: ' + str(len(y_train)))\n",
    "print('# of obs in test set: ' + str(len(y_test)))\n",
    "\n",
    "#train_defaults = sum(y_train == 0)\n",
    "train_defaults = sum(y_train == 1)\n",
    "print('# of defaults in train set: ' + str(train_defaults))\n",
    "\n",
    "proportion_train_defaults = train_defaults / len(y_train)\n",
    "print('Proportion of defaults in train set: ' + str(proportion_train_defaults))\n",
    "\n",
    "#test_defaults = sum(y_test == 0)\n",
    "test_defaults = sum(y_test == 1)\n",
    "print('# of defaults in test set: ' + str(test_defaults))\n",
    "\n",
    "proportion_test_defaults = test_defaults / len(y_test)\n",
    "print('Proportion of defaults in test set: ' + str(proportion_test_defaults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "credit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Logistic-Regression\"></a>Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We first run a logistic regression scenario using l2 normal penalty, a cost of 1 and assigning the same weight to the classes in the data.\n",
    "\n",
    ">Given the fact that the data is unbalanced, we are presenting the metrics derived from the confusion matrix (e.g. Precision, Recall, f1-score). A higher value of the metric \"Recall\" indicates that the model is predicting most defaults customers correctly.\n",
    "\n",
    ">Additionally, it is measured the time to fit and train the model to determine the cost of execution.\n",
    "\n",
    ">The performance of this first configuration is very poor since the \"Recall\" value for the category 1 (defaulted customers) is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from time import time\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "times_rec=[]\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    \n",
    "    #we count the time in executing the logistic regression\n",
    "    t0 = time()\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "    \n",
    "    t1=time()\n",
    "    diff=np.array([t1-t0])\n",
    "    \n",
    "    print (\"The time it takes to fit and predict is \" + str(diff[0]) + \"\\n\")    \n",
    "    times_rec=np.append(times_rec,diff)\n",
    "    \n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "    ClassReport = mt.classification_report(y_test,y_hat)\n",
    "    print(ClassReport)\n",
    "    \n",
    "    \n",
    "print(\"The average time to fit and predict 3 logistic regressions with 80/20 training/test split is: \" + str(times_rec.mean()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Adjust-Parameters-LR\"></a>Adjust Parameters for Logistic Regression\n",
    "\n",
    "> Let's adjust some parameters to see if a more accurate model can be produced.\n",
    "\n",
    "> By default, the LogisticRegression function within the SciKit Library gives all classes a weight of one. The class_weight parameter will be changed to 'balanced' to see if the accuracy of the models can be improved.\n",
    "\n",
    "> The results that follow, show an improvement in the performance, the \"Recall\" value is higher than 0.5, which is deemed adequate for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(penalty='l2', C=1, class_weight='balanced') # get object\n",
    "#WARNING: THE FIRST WIEGHT WAS 1 \n",
    "iter_num=0\n",
    "times_rec=[]\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "     #we count the time in executing the logistic regression\n",
    "    t0 = time()\n",
    "    \n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "    \n",
    "    t1=time()\n",
    "    diff=np.array([t1-t0])\n",
    "    \n",
    "    print (\"The time it takes to fit and predict is \" + str(diff[0]) + \"\\n\")    \n",
    "    times_rec=np.append(times_rec,diff)\n",
    "    \n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    \n",
    "    print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "    ClassReport = mt.classification_report(y_test,y_hat)\n",
    "    print(ClassReport)\n",
    "    \n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print(\"The average time to fit and predict 3 logistic regressions with 80/20 training/test split is: \" + str(times_rec.mean()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, we will change the normal penalty to be L1 and assigning the same weight to the categories by setting the parameter \"class_weight\" to \"None.\n",
    "\n",
    "> By default, the LogisticRegression function within the SciKit Learn library uses 'L2' as its 'penalty' parameter. A binary class L2 penalized logistic regression minimizes the following cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='http://scikit-learn.org/stable/_images/math/760c999ccbc78b72d2a91186ba55ce37f0d2cf37.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now the 'penalty' parameter will be changed to L1 in order to optimize the following problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='http://scikit-learn.org/stable/_images/math/6a0bcf21baaeb0c2b879ab74fe333c0aab0d6ae6.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(penalty='l1', C=1.0, class_weight=None) # get object\n",
    "\n",
    "iter_num=0\n",
    "times_rec=[]\n",
    "\n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    \n",
    "    #we count the time in executing the logistic regression\n",
    "    t0 = time()\n",
    "    \n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "    \n",
    "    t1=time()\n",
    "    diff=np.array([t1-t0])\n",
    "    \n",
    "    print (\"The time it takes to fit and predict is \" + str(diff[0]) + \"\\n\")    \n",
    "    times_rec=np.append(times_rec,diff)\n",
    "    \n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    \n",
    "    print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "    ClassReport = mt.classification_report(y_test,y_hat)\n",
    "    print(ClassReport)\n",
    "    iter_num+=1\n",
    "    \n",
    "print(\"The average time to fit and predict 3 logistic regressions with 80/20 training/test split is: \" + str(times_rec.mean()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The use of L1 penalty did not improve the performance of the model since the value of \"Recall\" metric decreased. \n",
    "\n",
    "> Therefore, the best scenario tested at this point is using L2 normal penalty and setting the class_weight parameter to be “balanced”. The latter uses the values of response variable to automatically adjust weights inversely proportional to class frequencies in the input data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <a name=\"Interpreting-Weights\"></a>Interpreting Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and here is an even shorter way of getting the accuracies for each training and test set\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(lr_clf, X, y=y, cv=cv_object) # this also can help with parallelism\n",
    "print(accuracies)\n",
    "\n",
    "\n",
    "# here we can change some of the parameters interactively\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def lr_explor(cost):\n",
    "    lr_clf = LogisticRegression(penalty='l2', C=cost, class_weight='balanced') # get object\n",
    "    accuracies = cross_val_score(lr_clf,X,y=y,cv=cv_object) # this also can help with parallelism\n",
    "    print(accuracies)\n",
    "\n",
    "    #WARNING=cost=(0.001,5.0,0.05)\n",
    "wd.interact(lr_explor,cost=(0.001,5.0,0.05),__manual=True)\n",
    "\n",
    "\n",
    "# interpret the weights\n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = df.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These weight interpretations are not necessarily interpretable because of the values we had. Very large attribute values could just as easily be assigned a higher weight. Instead, let's normalize the feature values so that all the attributes are on the same dynamic range. Once we normalize the attributes, the weights should have magnitudes that reflect their predictive power in the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1, class_weight='balanced') # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,df.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=df.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydoc import help\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "#correlation with in like variables \n",
    "bill = pearsonr(credit.BILL_AMT1, credit.BILL_AMT2)\n",
    "pay = pearsonr(credit.PAY_AMT1, credit.PAY_AMT2)\n",
    "\n",
    "\n",
    "print('bill amount correlation:', bill)\n",
    "print('pay amount correlation:', pay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> For more improvement and guarding against overfitting: At this point it make sense to remove variables highly related to one another and irrelevant ones and keep going with the weights analysis.\n",
    "\n",
    "> Initally only a handful of the explanatory variables seem to be worth keeping in the model. In particular, PAY_1, BILL_AMT1, BILL_AMT2, PAY_AMT1 and PAY_AMT2 have the greatest absolute weights. After taking a look at the correlations between BILL_AMT1 and BILL_AMT2 , with a correlation above .95, we can delete BILL_AMT2 to prevent over fitting. Like wise, we can keep both PAY_AMT1 and PAY_AMT2 becasue of the low correlation of .285.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation with response variable\n",
    "gender = pearsonr(credit.SEX, credit.default_next_m)\n",
    "Marrige = pearsonr(credit.MARRIAGE_INP, credit.default_next_m)\n",
    "age = pearsonr(credit.AGE, credit.default_next_m)\n",
    "education = pearsonr(credit.EDUCATION_INP, credit.default_next_m)\n",
    "#billD = pearsonr(credit.BILL_AMT1, credit.default_next_m)\n",
    "#payD1 = pearsonr(credit.PAY_AMT1, credit.default_next_m)\n",
    "#payD2 = pearsonr(credit.PAY_AMT2, credit.default_next_m)\n",
    "print('age default correlation:', age)\n",
    "print('marrige default correlation:', Marrige)\n",
    "print('gender default correlation:', gender)\n",
    "print('education default correlation:', education)\n",
    "#print('bill default correlation:', billD)\n",
    "#print('pay1 default correlation:', payD1)\n",
    "#print('pay2 default correlation:', payD2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Furthermore, Education, Marriage, Age and Sex appears to have only marginal significance in the model, but it could be argued to keep them in the model if all redundant PAY_X, BILL_AMTX, and PAY_AMTX variables are being removed. The argument to keep these variables in the model would be to help classify people before they have a payment or bill history with the company. For instance, a company could offer customer different products or marketing campaigns based on categories they would fall in if we used the attributes in our model.\n",
    "\n",
    "> After taking a further look at these \"profiling\" features, if seems reasonable to keep the top two features with the highest weights, Marriage and Education, in our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# you can apply the StandardScaler function inside of the cross-validation loop \n",
    "#  but this requires the use of PipeLines in scikit. \n",
    "#  A pipeline can apply feature pre-processing and data fitting in one compact notation\n",
    "#  Here is an example!\n",
    "\n",
    "std_scl = StandardScaler()\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1, class_weight='balanced') \n",
    "\n",
    "# create the pipline\n",
    "piped_object = Pipeline([('scale', std_scl),  # do this\n",
    "                         ('logit_model', lr_clf)]) # and then do this\n",
    "\n",
    "weights = []\n",
    "# run the pipline cross validated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    piped_object.fit(X[train_indices],y[train_indices])  # train object\n",
    "    # it is a little odd getting trained objects from a  pipeline:\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "\n",
    "weights = np.array(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from plotly import __version__\n",
    "#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "#plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "print(\"Plotly version: \" + plotly.__version__)           # version 1.9.x required\n",
    "plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': df.columns,\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = df[['PAY_1','BILL_AMT1','PAY_AMT1','PAY_AMT2','EDUCATION_INP','MARRIAGE_INP']].values\n",
    "\n",
    "weights = []\n",
    "# run the pipline corssvalidated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(Xnew,y)):\n",
    "    piped_object.fit(Xnew[train_indices],y[train_indices])  # train object\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "weights = np.array(weights)\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': ['PAY_1','BILL_AMT1','PAY_AMT1','PAY_AMT2','EDUCATION_INP','MARRIAGE_INP'],\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At first glance one might think each bill and payment period should have equal weight within the logistic regression model. However, clearly the first bill and payment period have the most significance upon viewing the above plots. Perhaps the greater weight of the first period compared to the other five can be explained by the recency of the period. \n",
    "\n",
    "> Simply put, if a customer fails to pay their credit bill in the first opportunity to pay, they are marked as having defaulted in the dataset. In other words, if a customer is likely to default on their bills, then the first chance for said customer is the first period.\n",
    "\n",
    "> If a customer has paid duly for periods 1-5, then they are likely to also pay their credit bill on the 6th go around, thus exhibiting they are fiscally sound as a customer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Support-Vector-Machines\"></a>Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "# Big Data, in all its glory:\n",
    "Image(url='http://flowingdata.com/wp-content/uploads/2014/11/Big-Data-620x465.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In this section, we will explore different configurations of SVM to derive a model to classify defaulted and non-defaulted customers.\n",
    "\n",
    "> As we did for the logistic regression, we are presenting the metrics derived from the confusion matrix (e.g. Precision, Recall, f1-score) since our data in unbalanced. A higher value of the metric \"Recall\" indicates that the model is predicting most defaults customers correctly. Lastly, the time to fit and predict is also measured.\n",
    "\n",
    ">The first configuration tested uses a \"linear\" kernel with a cost of 0.5 (C-parameter) a degree of 3 and gamma set to auto.\n",
    "\n",
    ">According to sklearn documentation:\n",
    "\n",
    ">Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.\n",
    "\n",
    ">The C parameter trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly by giving the model freedom to select more samples as support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, so run through the cross validation loop and set the training and testing variable for one single iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels - CHANGING THE KERNEL TO BE LINEAR\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "#warning\n",
    "#svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "svm_clf = SVC(C=0.5, kernel='linear', degree=3, gamma='auto') # get object\n",
    "\n",
    "#we count the time in executing the logistic regression\n",
    "t0 = time()\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "t1=time()\n",
    "diff=t1-t0\n",
    "    \n",
    "print (\"The time it takes to fit and predict is \" + str(diff) + \"\\n\")  \n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)\n",
    "\n",
    "\n",
    "print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "ClassReport = mt.classification_report(y_test,y_hat)\n",
    "print(ClassReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The value of recall exhibited above is of 0.23 which is considerably lower than the best scenario of the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, keeping other parameters equal, we use the Radial Basis Function (RBF) kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels - CHANGING THE KERNEL TO BE RBF\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "#warning\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "#svm_clf = SVC(C=0.5, kernel='linear', degree=3, gamma='auto') # get object\n",
    "\n",
    "#we count the time in executing the logistic regression\n",
    "t0 = time()\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "t1=time()\n",
    "diff=t1-t0\n",
    "    \n",
    "print (\"The time it takes to fit and predict is \" + str(diff) + \"\\n\")  \n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)\n",
    "\n",
    "\n",
    "print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "ClassReport = mt.classification_report(y_test,y_hat)\n",
    "print(ClassReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The results above show a higher value of the \"Recall\" metric which in this case indicates an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Following with the configuration presented above, now we change the parameter C equals to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels - CHANGING THE KERNEL TO BE RBF\n",
    "#AND CHANGING THE COST TO BE 2\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "#warning\n",
    "svm_clf = SVC(C=2, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "#svm_clf = SVC(C=0.5, kernel='linear', degree=3, gamma='auto') # get object\n",
    "\n",
    "#we count the time in executing the logistic regression\n",
    "t0 = time()\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "t1=time()\n",
    "diff=t1-t0\n",
    "    \n",
    "print (\"The time it takes to fit and predict is \" + str(diff) + \"\\n\")  \n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)\n",
    "\n",
    "\n",
    "print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "ClassReport = mt.classification_report(y_test,y_hat)\n",
    "print(ClassReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The results above show a higher value of the \"Recall\" metric which in this case indicates an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Using the latest configuration, now we change the parameter gamma to be 1e1. This change does not improve the performance of the model as the results below exhibit since the value of the recall decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels - CHANGING THE KERNEL TO BE RBF\n",
    "#AND CHANGING THE WEIGHT TO BE 2 AND GAMMA=1e1\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "#warning\n",
    "svm_clf = SVC(C=2, kernel='rbf', degree=3, gamma=1e1) # get object\n",
    "#svm_clf = SVC(C=0.5, kernel='linear', degree=3, gamma='auto') # get object\n",
    "\n",
    "#we count the time in executing the logistic regression\n",
    "t0 = time()\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "t1=time()\n",
    "diff=t1-t0\n",
    "    \n",
    "print (\"The time it takes to fit and predict is \" + str(diff) + \"\\n\")  \n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)\n",
    "\n",
    "\n",
    "print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "ClassReport = mt.classification_report(y_test,y_hat)\n",
    "print(ClassReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The best configuration identified at this point for SVM uses the \"rbf\" kernel with a cost of 2, degree of 3 and the gamma parameter equal to 2. To validate the performance of this configuration, to avoid overfitting we execute a cross-validation with 3 folds. The results in the \"recall\" metric are very similar, as exhibited below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### CV FOR SVM TO VALIDATE OVERFITTING #######################\n",
    "\n",
    "# we are using the best scenario to run the SVM AFTER changing some of the parameters\n",
    "#lr_clf = LogisticRegression(penalty='l1', C=1.0, class_weight=None) # get object\n",
    "svm_clf = SVC(C=2, kernel='rbf', degree=3, gamma='auto')\n",
    "\n",
    "\n",
    "iter_num=0\n",
    "times_rec=[]\n",
    "\n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    #### SCALING THE  FOLD\n",
    "    #OBTAINING THE TUNING PARAMETERS FOR EACHV ARIABLE IN THE TRAINING SAMPLE\n",
    "    scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "    # the line of code above only looks at training data to get mean and std and we can use it \n",
    "    # to transform new feature data\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "    X_test_scaled = scl_obj.transform(X_test)\n",
    "    \n",
    "    \n",
    "    #we count the time in executing the logistic regression\n",
    "    t0 = time()\n",
    "    \n",
    "    svm_clf.fit(X_train_scaled,y_train)  # train object\n",
    "    y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "    \n",
    "    t1=time()\n",
    "    diff=np.array([t1-t0])\n",
    "    \n",
    "    print (\"The time it takes to fit and predict is \" + str(diff[0]) + \"\\n\")    \n",
    "    times_rec=np.append(times_rec,diff)\n",
    "    \n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    \n",
    "    print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "    ClassReport = mt.classification_report(y_test,y_hat)\n",
    "    print(ClassReport)\n",
    "    iter_num+=1\n",
    "    \n",
    "print(\"The average time to fit and predict 3 SVM with 80/20 training/test split is: \" + str(times_rec.mean()) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Chosen-Support-Vectors-Analysis\"></a>Chosen Support Vectors Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this section, we take a look at the chosen support vectors (with the best scenario obtained in the prior section) for the classification task in order to identify any insight into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ FOR THE CONFIGURATION SELECTED WE NEED TO RE-RUN THE CODE TO DO THE ANALYSIS OF\n",
    "###### CHOSEN SUPPORT VECTORS\n",
    "\n",
    "# lets investigate SVMs on the data and play with the parameters and kernels - CHANGING THE KERNEL TO BE RBF\n",
    "#AND CHANGING THE COST TO BE 2\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "#warning\n",
    "svm_clf = SVC(C=2, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "#svm_clf = SVC(C=0.5, kernel='linear', degree=3, gamma='auto') # get object\n",
    "\n",
    "#we count the time in executing the logistic regression\n",
    "t0 = time()\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "t1=time()\n",
    "diff=t1-t0\n",
    "    \n",
    "print (\"The time it takes to fit and predict is \" + str(diff) + \"\\n\")  \n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)\n",
    "\n",
    "\n",
    "print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "ClassReport = mt.classification_report(y_test,y_hat)\n",
    "print(ClassReport)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do some different analysis with the SVM and look at the instances that were chosen as support vectors\n",
    "\n",
    "# now lets look at the support for the vectors and see if we they are indicative of anything\n",
    "# grabe the rows that were selected as support vectors (these are usually instances that are hard to classify)\n",
    "\n",
    "# make a dataframe of the training data\n",
    "df_tested_on = df.iloc[train_indices] # saved from above, the indices chosen for training\n",
    "# now get the support vectors from the trained model\n",
    "df_support = df_tested_on.iloc[svm_clf.support_,:]\n",
    "\n",
    "df_support['default_next_m'] = y[svm_clf.support_] # add back in the 'Survived' Column to the pandas dataframe\n",
    "df['default_next_m'] = y # also add it back in for the original data\n",
    "df_support.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following charts exhibit the KDE broke down by defaulted and non-defaulted customers. The charts on the left show the chosen support vectors and the charts on the right present the distribution of the variable. At the end, Support Vectors are simply the coordinates of individual observation. A Support Vector is a frontier which best segregates the two classes (hyper-plane/ line).\n",
    "\n",
    ">For example, the variable \"Pay_1\", the value of \"0\" shows a smaller gap between defaulted and non-defaulted, in other words, we are seeing the observations that are deemed as an error and used by the SVM to build the frontier.\n",
    "\n",
    ">The distributions for the variable \"Marriage_INP\" between the chosen support vectors and the real data looks very similar, that is telling us that there were not many instances identified as \"errors\" and therefore selected as Support Vectors based on the values for this variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets see the statistics of these attributes\n",
    "from pandas.tools.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby(['default_next_m'])\n",
    "df_grouped = df.groupby(['default_next_m'])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['PAY_1','BILL_AMT1','PAY_AMT1','EDUCATION_INP','MARRIAGE_INP']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['Default','Paid Duly'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['Default','Paid Duly'])\n",
    "    plt.title(v+' (Original)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By plotting the original data density statistics next to the statistics for the support vectors, we can look at the separation between the distributions of the defaults and those that paid duly. Generally, the separations between the original data will be larger than the separations from the support vector data, since the support vectors use points on the decision boundary to create the dividing line for classification. Otherwise, the points that are used are errors. Since the support vectors use edge points, the kernel density estimation lines should appear close together. And, the data for the PAY_1 and BILL_AMT1 variables do indeed have smaller separations between the distributions for the support vectors. This development is consistent with the weights calculated previously. Since PAY_1 and BILL_AMT1 are so much more significant to the model predictions than any of the other variables, it makes sense that these two variables make up many of the support vectors. Since more data points using those two variables are used from the original data from the support vectors, the distribution between two is a closer match, as seen in the charts.\n",
    "\n",
    "> However, the three remaining variables – PAY_AMT1, EDUCATION_INP, and MARRIAGE_INP - have similar or even slightly larger separations between the distributions. Since these variables are not weighted as heavily as the two primary prediction variables, the data diverges slightly between the original and support vector plots. The separation can vary depending on the size of the margins. These values could have error values that negatively impact the KDE lines that are drawn between the original and support vector data, leading to the slightly increased separations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint Plot\n",
    "# Source:\n",
    "# http://seaborn.pydata.org/generated/seaborn.jointplot.html\n",
    "import seaborn as sns\n",
    "\n",
    "# Original dataset\n",
    "g = sns.jointplot(\"MARRIAGE_INP\", \"PAY_1\", data=df, kind=\"kde\", space=0, color=\"g\")\n",
    "\n",
    "# Support Vector\n",
    "g = sns.jointplot(\"MARRIAGE_INP\", \"PAY_1\", data=df_support, kind=\"kde\", space=0, color=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Looking at the bivariate distribution of the two variables, the correlation between the PAY_1 variable and the MARRIAGE_INP improves in the support vector data over the original data. As seen in the plots, the density around all combinations of points improve and is most noticeable around the central points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint Plot\n",
    "# Source:\n",
    "# http://seaborn.pydata.org/generated/seaborn.jointplot.html\n",
    "import seaborn as sns;\n",
    "x = df_support['MARRIAGE_INP']\n",
    "y = df_support['PAY_1']\n",
    "g = (sns.jointplot(x, y, kind=\"hex\", stat_func=None).set_axis_labels(\"Marriage\", \"Pay_1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Taking another look at the PAY_1 and MARRIAGE_INP variables for the support vector data, the density improvement around the central values can be seen in the very dark points in the joint histogram using hexagonal bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(\"BILL_AMT1\", \"PAY_AMT1\", data=df, kind=\"hex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># <a name=\"Advantages-of-each-model\"></a>Advantages of each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For this classification problem, we recommend to use the logistic regression based upon the following:\n",
    "    \n",
    "       >1)The classification metrics of \"Recall\" and \"F1-score\" are higher for the logistic regression. The data is unbalanced and therefore, the \"Recall\" is a better metric to measure the performance of the model since a high value means that it is predicting more default customers correctly. \n",
    "       \n",
    "       >2) The time of execution is considerably higher for SVM. On average, the time taken to fit and predict the model is around 1 second using logistic regression whereas the SVM takes around 30 seconds. \n",
    "       \n",
    "       >3) The weights of the logistic regression provide a method to derive the importance of each variable. The bank running this model would be interested to know what variables are key to identify defaults to create segments that allow them to offer customer different products or marketing campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <a name=\"References\"></a>References\n",
    "\n",
    "* https://github.com/eclarson/DataMiningNotebooks/blob/master/04.%20Logits%20and%20SVM.ipynb\n",
    "* https://github.com/jakemdrew/EducationDataNC/blob/master/Graduation%20Rates%20v2.ipynb\n",
    "* http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "* https://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn\n",
    "* http://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/\n",
    "* https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
