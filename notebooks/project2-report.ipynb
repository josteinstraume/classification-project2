{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "## MSDS 7349 - Section 401\n",
    "## Project 2\n",
    "\n",
    "[Data Science @ Southern Methodist University](https://datascience.smu.edu/)\n",
    "\n",
    "# Table of Contents\n",
    "* [Team Members](#Team-Members)\n",
    "* [Data Preparation](#Data-Preparation)\n",
    "* [Dataset Description](#Dataset-Description)\n",
    "* [Evaluation Metrics Description](#Evaluation-Metrics-Description)\n",
    "* [Training and Testing Splits method](#Training-Testing)\n",
    "* [Classification Task](#Classification-Task)\n",
    "* [Regression Task](#Regression-Task)\n",
    "* [Methods of Evaluation Results](#Methods-of-Evaluation)\n",
    "* [Advantages of each model](#Advantages-of-each-model)\n",
    "* [Relevant Attributes](#Relevant-Attributes)\n",
    "* [Deployment](#Deployment)\n",
    "* [Exceptional Work](#Exceptional-Work)\n",
    "* [References](#References)\n",
    "\n",
    "\n",
    "# <a name=\"Team-Members\"></a>Team Members\n",
    "* [Jostein Barry-Straume](https://github.com/josteinstraume)\n",
    "* [Kevin Cannon](https://github.com/kcannon2)\n",
    "* [Ernesto Carrera Ruvalcaba](https://github.com/ecarrerasmu)\n",
    "* [Adam Tschannen](https://github.com/adamtschannen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Data-Preparation\"></a>Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[10 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "\n",
    "> The credit default data set is from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients#). It has 30,000 records with 24 attributes. The records reflect the payment data from an important Taiwanese bank during October, 2005 (Yeh & Lien 2475).\n",
    "\n",
    "> The purpose of the data set was to tackle the issue of forecasting the probability of default. In particular, the researchers of this data set sought to determine if the \"estimated probability of default produced from data mining methods... (could) represent the \"real\" probability of default\" (Yeh & Lien 2473).\n",
    "\n",
    "> The dataset needs some pre-processing before it is ready for classification and regression analysis.\n",
    "\n",
    "> To start, missing or incorrect values for education level, marital status, and repayment status are imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import statistics as st\n",
    "import csv as csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from __future__ import print_function\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "filepath = \"/data/credit-defaults.xls\"\n",
    "\n",
    "# Relative local path to avoid changing filepath constantly\n",
    "credit = pd.read_excel('../data/credit-defaults.xls', header=1, skiprows=0)\n",
    "\n",
    "\n",
    "# Rename column(s)\n",
    "credit = credit.rename(columns={'default payment next month': 'default_next_m', 'PAY_0': 'PAY_1'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING NEW VARIABLE TO IMPUTE THE VALUES TO 4, THAT REPRESENTS OTHER\n",
    "credit['EDUCATION_INP']=credit['EDUCATION']\n",
    "\n",
    "credit.loc[credit['EDUCATION'] > 4, 'EDUCATION_INP'] = 4\n",
    "credit.loc[credit['EDUCATION'] == 0, 'EDUCATION_INP'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "####FOR MARRIAGE\n",
    "#CREATING NEW VARIABLE TO IMPUTE THE VALUES TO 2, THAT REPRESETNS SINGLE, since the value of 0 is not defined in\n",
    "#the data dictionary\n",
    "credit['MARRIAGE_INP']=credit['MARRIAGE']\n",
    "\n",
    "\n",
    "credit.loc[credit['MARRIAGE'] == 0, 'MARRIAGE_INP'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### CREATING A FUNCTION TO GROUP VARIABLES\n",
    "def group_biv_v1(var_name,RangeName,CutOff,NumBins,BinLabel,var_x,var_y,title1,title2):\n",
    "#def bivariate_v1(NumBins): \n",
    "    #Creating the BINS\n",
    "    credit[RangeName]=pd.cut(var_name,CutOff,NumBins,labels=BinLabel)\n",
    "    #Grouping by the BINS variable\n",
    "    default_g=credit.groupby(by=[RangeName])\n",
    "    #% of POPULATION\n",
    "    bins_percentage=default_g[RangeName].count()/credit[RangeName].count()\n",
    "    \n",
    "    #### creating DUMMY VARIABLES\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#group_biv_v1(credit.PAY_1,'PAY_1_range',[-2.1,0,1,10],3,['Pay duly','delay 1 month ','delay > 1'],'Percentage','PAY_1_bins','% of population PAY_1','Default PAY_1 ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## IMPUTE VALUES FOR THE VARIABLES OF REPAYMENT STATUS (PAY0, PAY1,   PAYN)\n",
    "group_biv_v1(credit.PAY_1,'PAY_1_range',[-2.1,0,1,10],3,['Pay duly','delay 1 month ','delay > 1'],'Percentage','PAY_1_bins','% of population PAY_1','Default PAY_1 ')\n",
    "\n",
    "group_biv_v1(credit.PAY_2,'PAY_2_range',[-2.1,1,10],2,['Pay duly','delay > 1'],'Percentage','PAY_2_bins','% of population PAY_2','Default PAY_2 ')\n",
    "\n",
    "group_biv_v1(credit.PAY_3,'PAY_3_range',[-2.1,1,10],2,['Pay duly','delay > 1'],'Percentage','PAY_3_bins','% of population PAY_3','Default PAY_3 ')\n",
    "\n",
    "group_biv_v1(credit.PAY_4,'PAY_4_range',[-2.1,1,10],2,['Pay duly','delay > 1'],'Percentage','PAY_4_bins','% of population PAY_4','Default PAY_4 ')\n",
    "\n",
    "group_biv_v1(credit.PAY_5,'PAY_5_range',[-2.1,1,10],2,['Pay duly','delay > 1'],'Percentage','PAY_5_bins','% of population PAY_5','Default PAY_5 ')\n",
    "\n",
    "group_biv_v1(credit.PAY_6,'PAY_6_range',[-2.1,1,10],2,['Pay duly','delay > 1'],'Percentage','PAY_6_bins','% of population PAY_6','Default PAY_6 ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_1</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default_next_m</th>\n",
       "      <th>EDUCATION_INP</th>\n",
       "      <th>MARRIAGE_INP</th>\n",
       "      <th>PAY_1_range</th>\n",
       "      <th>PAY_2_range</th>\n",
       "      <th>PAY_3_range</th>\n",
       "      <th>PAY_4_range</th>\n",
       "      <th>PAY_5_range</th>\n",
       "      <th>PAY_6_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>delay &gt; 1</td>\n",
       "      <td>delay &gt; 1</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>delay &gt; 1</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>delay &gt; 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "      <td>Pay duly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_1  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1      20000    2          2         1   24      2      2     -1     -1   \n",
       "1   2     120000    2          2         2   26     -1      2      0      0   \n",
       "2   3      90000    2          2         2   34      0      0      0      0   \n",
       "3   4      50000    2          2         1   37      0      0      0      0   \n",
       "4   5      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "      ...       PAY_AMT6  default_next_m  EDUCATION_INP  MARRIAGE_INP  \\\n",
       "0     ...              0               1              2             1   \n",
       "1     ...           2000               1              2             2   \n",
       "2     ...           5000               0              2             2   \n",
       "3     ...           1000               0              2             1   \n",
       "4     ...            679               0              2             1   \n",
       "\n",
       "   PAY_1_range  PAY_2_range  PAY_3_range  PAY_4_range  PAY_5_range  \\\n",
       "0    delay > 1    delay > 1     Pay duly     Pay duly     Pay duly   \n",
       "1     Pay duly    delay > 1     Pay duly     Pay duly     Pay duly   \n",
       "2     Pay duly     Pay duly     Pay duly     Pay duly     Pay duly   \n",
       "3     Pay duly     Pay duly     Pay duly     Pay duly     Pay duly   \n",
       "4     Pay duly     Pay duly     Pay duly     Pay duly     Pay duly   \n",
       "\n",
       "   PAY_6_range  \n",
       "0     Pay duly  \n",
       "1    delay > 1  \n",
       "2     Pay duly  \n",
       "3     Pay duly  \n",
       "4     Pay duly  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 33 columns):\n",
      "ID                30000 non-null int64\n",
      "LIMIT_BAL         30000 non-null int64\n",
      "SEX               30000 non-null int64\n",
      "EDUCATION         30000 non-null int64\n",
      "MARRIAGE          30000 non-null int64\n",
      "AGE               30000 non-null int64\n",
      "PAY_1             30000 non-null int64\n",
      "PAY_2             30000 non-null int64\n",
      "PAY_3             30000 non-null int64\n",
      "PAY_4             30000 non-null int64\n",
      "PAY_5             30000 non-null int64\n",
      "PAY_6             30000 non-null int64\n",
      "BILL_AMT1         30000 non-null int64\n",
      "BILL_AMT2         30000 non-null int64\n",
      "BILL_AMT3         30000 non-null int64\n",
      "BILL_AMT4         30000 non-null int64\n",
      "BILL_AMT5         30000 non-null int64\n",
      "BILL_AMT6         30000 non-null int64\n",
      "PAY_AMT1          30000 non-null int64\n",
      "PAY_AMT2          30000 non-null int64\n",
      "PAY_AMT3          30000 non-null int64\n",
      "PAY_AMT4          30000 non-null int64\n",
      "PAY_AMT5          30000 non-null int64\n",
      "PAY_AMT6          30000 non-null int64\n",
      "default_next_m    30000 non-null int64\n",
      "EDUCATION_INP     30000 non-null int64\n",
      "MARRIAGE_INP      30000 non-null int64\n",
      "PAY_1_range       30000 non-null category\n",
      "PAY_2_range       30000 non-null category\n",
      "PAY_3_range       30000 non-null category\n",
      "PAY_4_range       30000 non-null category\n",
      "PAY_5_range       30000 non-null category\n",
      "PAY_6_range       30000 non-null category\n",
      "dtypes: category(6), int64(27)\n",
      "memory usage: 6.4 MB\n"
     ]
    }
   ],
   "source": [
    "credit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 37 columns):\n",
      "ID                            30000 non-null int64\n",
      "LIMIT_BAL                     30000 non-null int64\n",
      "EDUCATION                     30000 non-null int64\n",
      "MARRIAGE                      30000 non-null int64\n",
      "AGE                           30000 non-null int64\n",
      "PAY_1                         30000 non-null int64\n",
      "PAY_2                         30000 non-null int64\n",
      "PAY_3                         30000 non-null int64\n",
      "PAY_4                         30000 non-null int64\n",
      "PAY_5                         30000 non-null int64\n",
      "PAY_6                         30000 non-null int64\n",
      "BILL_AMT1                     30000 non-null int64\n",
      "BILL_AMT2                     30000 non-null int64\n",
      "BILL_AMT3                     30000 non-null int64\n",
      "BILL_AMT4                     30000 non-null int64\n",
      "BILL_AMT5                     30000 non-null int64\n",
      "BILL_AMT6                     30000 non-null int64\n",
      "PAY_AMT1                      30000 non-null int64\n",
      "PAY_AMT2                      30000 non-null int64\n",
      "PAY_AMT3                      30000 non-null int64\n",
      "PAY_AMT4                      30000 non-null int64\n",
      "PAY_AMT5                      30000 non-null int64\n",
      "PAY_AMT6                      30000 non-null int64\n",
      "default_next_m                30000 non-null int64\n",
      "EDUCATION_INP_2               30000 non-null uint8\n",
      "EDUCATION_INP_3               30000 non-null uint8\n",
      "EDUCATION_INP_4               30000 non-null uint8\n",
      "MARRIAGE_INP_2                30000 non-null uint8\n",
      "MARRIAGE_INP_3                30000 non-null uint8\n",
      "PAY_1_range_delay 1 month     30000 non-null uint8\n",
      "PAY_1_range_delay > 1         30000 non-null uint8\n",
      "PAY_2_range_delay > 1         30000 non-null uint8\n",
      "PAY_3_range_delay > 1         30000 non-null uint8\n",
      "PAY_4_range_delay > 1         30000 non-null uint8\n",
      "PAY_5_range_delay > 1         30000 non-null uint8\n",
      "PAY_6_range_delay > 1         30000 non-null uint8\n",
      "SEX_2                         30000 non-null uint8\n",
      "dtypes: int64(24), uint8(13)\n",
      "memory usage: 5.9 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_1</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>MARRIAGE_INP_2</th>\n",
       "      <th>MARRIAGE_INP_3</th>\n",
       "      <th>PAY_1_range_delay 1 month</th>\n",
       "      <th>PAY_1_range_delay &gt; 1</th>\n",
       "      <th>PAY_2_range_delay &gt; 1</th>\n",
       "      <th>PAY_3_range_delay &gt; 1</th>\n",
       "      <th>PAY_4_range_delay &gt; 1</th>\n",
       "      <th>PAY_5_range_delay &gt; 1</th>\n",
       "      <th>PAY_6_range_delay &gt; 1</th>\n",
       "      <th>SEX_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  EDUCATION  MARRIAGE  AGE  PAY_1  PAY_2  PAY_3  PAY_4  PAY_5  \\\n",
       "0   1      20000          2         1   24      2      2     -1     -1     -2   \n",
       "1   2     120000          2         2   26     -1      2      0      0      0   \n",
       "2   3      90000          2         2   34      0      0      0      0      0   \n",
       "3   4      50000          2         1   37      0      0      0      0      0   \n",
       "4   5      50000          2         1   57     -1      0     -1      0      0   \n",
       "\n",
       "   ...    MARRIAGE_INP_2  MARRIAGE_INP_3  PAY_1_range_delay 1 month   \\\n",
       "0  ...                 0               0                           0   \n",
       "1  ...                 1               0                           0   \n",
       "2  ...                 1               0                           0   \n",
       "3  ...                 0               0                           0   \n",
       "4  ...                 0               0                           0   \n",
       "\n",
       "   PAY_1_range_delay > 1  PAY_2_range_delay > 1  PAY_3_range_delay > 1  \\\n",
       "0                      1                      1                      0   \n",
       "1                      0                      1                      0   \n",
       "2                      0                      0                      0   \n",
       "3                      0                      0                      0   \n",
       "4                      0                      0                      0   \n",
       "\n",
       "   PAY_4_range_delay > 1  PAY_5_range_delay > 1  PAY_6_range_delay > 1  SEX_2  \n",
       "0                      0                      0                      0      1  \n",
       "1                      0                      0                      1      1  \n",
       "2                      0                      0                      0      1  \n",
       "3                      0                      0                      0      1  \n",
       "4                      0                      0                      0      0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### CREATE DUMMY VARIABLES\n",
    "#Convert Categorical fields to bool using get_dummies\n",
    "#Use drop_first=true to aviod multicolinierarity\n",
    "### The drop_first=true eliminates the first group of the dummy variable\n",
    "### According to the notes provided of the professor we do this to prevent multicollinearity\n",
    "#### I can see it in the context of regression and logistic regression. We may want to select the group to delet\n",
    "##### The method \"get_dummies\" also ELIMINATES THE ORIGINAL VARIABLE TO CREATE THE dummies\n",
    "\n",
    "#EC: Creating  another reference in MEMORY, so the changes in df will not be REFLECTED IN CREDIT\n",
    "df = credit.copy()\n",
    "\n",
    "\n",
    "\n",
    "df = pd.get_dummies(data=credit, \n",
    "                       columns=['EDUCATION_INP',\n",
    "                                'MARRIAGE_INP', \n",
    "                                'PAY_1_range',\n",
    "                                'PAY_2_range',\n",
    "                                'PAY_3_range',\n",
    "                                'PAY_4_range',\n",
    "                                'PAY_5_range',\n",
    "                                'PAY_6_range',\n",
    "                                'SEX'], drop_first=True) #Try drop_first=true to aviod multicolinierarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "#credit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, additional variables are created to help with the regression and classification analysis. The following variables were created and added to the dataset:\n",
    "* Average bill amount over the 6 month period\n",
    "* Average payment amount over the 6 month period\n",
    "* Ratio of average bill amount to credit limit balance\n",
    "* Ratio of the average payment amount to credit limit balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 41 columns):\n",
      "ID                            30000 non-null int64\n",
      "LIMIT_BAL                     30000 non-null int64\n",
      "EDUCATION                     30000 non-null int64\n",
      "MARRIAGE                      30000 non-null int64\n",
      "AGE                           30000 non-null int64\n",
      "PAY_1                         30000 non-null int64\n",
      "PAY_2                         30000 non-null int64\n",
      "PAY_3                         30000 non-null int64\n",
      "PAY_4                         30000 non-null int64\n",
      "PAY_5                         30000 non-null int64\n",
      "PAY_6                         30000 non-null int64\n",
      "BILL_AMT1                     30000 non-null int64\n",
      "BILL_AMT2                     30000 non-null int64\n",
      "BILL_AMT3                     30000 non-null int64\n",
      "BILL_AMT4                     30000 non-null int64\n",
      "BILL_AMT5                     30000 non-null int64\n",
      "BILL_AMT6                     30000 non-null int64\n",
      "PAY_AMT1                      30000 non-null int64\n",
      "PAY_AMT2                      30000 non-null int64\n",
      "PAY_AMT3                      30000 non-null int64\n",
      "PAY_AMT4                      30000 non-null int64\n",
      "PAY_AMT5                      30000 non-null int64\n",
      "PAY_AMT6                      30000 non-null int64\n",
      "default_next_m                30000 non-null int64\n",
      "EDUCATION_INP_2               30000 non-null uint8\n",
      "EDUCATION_INP_3               30000 non-null uint8\n",
      "EDUCATION_INP_4               30000 non-null uint8\n",
      "MARRIAGE_INP_2                30000 non-null uint8\n",
      "MARRIAGE_INP_3                30000 non-null uint8\n",
      "PAY_1_range_delay 1 month     30000 non-null uint8\n",
      "PAY_1_range_delay > 1         30000 non-null uint8\n",
      "PAY_2_range_delay > 1         30000 non-null uint8\n",
      "PAY_3_range_delay > 1         30000 non-null uint8\n",
      "PAY_4_range_delay > 1         30000 non-null uint8\n",
      "PAY_5_range_delay > 1         30000 non-null uint8\n",
      "PAY_6_range_delay > 1         30000 non-null uint8\n",
      "SEX_2                         30000 non-null uint8\n",
      "Avg_BILL                      30000 non-null float64\n",
      "Avg_PAY                       30000 non-null float64\n",
      "AvgBill_to_LIMIT_BAL          30000 non-null float64\n",
      "AvgPay_to_LIMIT_BAL           30000 non-null float64\n",
      "dtypes: float64(4), int64(24), uint8(13)\n",
      "memory usage: 6.8 MB\n"
     ]
    }
   ],
   "source": [
    "##### CREATING ADDITIONAL VARIABLES \n",
    "\n",
    "############## CREATING NEW VARIABLES\n",
    "\n",
    "#Average of the bill amount\n",
    "df['Avg_BILL']=(df['BILL_AMT1']+df['BILL_AMT2']+df['BILL_AMT3']+ df['BILL_AMT4']+df['BILL_AMT5']+df['BILL_AMT6']\n",
    "               )/float(6)\n",
    "\n",
    "\n",
    "#Average of the Payment amount\n",
    "\n",
    "df['Avg_PAY']=(df['PAY_AMT1']+df['PAY_AMT2']+df['PAY_AMT3']+df['PAY_AMT4']+df['PAY_AMT5']+df['PAY_AMT6']\n",
    "               )/float(6)\n",
    "\n",
    "#We are constructing the ratio with the average BILL AMOUNT TO LIMIT\n",
    "df['AvgBill_to_LIMIT_BAL']=(df['Avg_BILL']/df['LIMIT_BAL'])\n",
    "\n",
    "\n",
    "#We are constructing the ratio with the  average PAYMENT AMOUNT TO LIMIT\n",
    "df['AvgPay_to_LIMIT_BAL']=(df['Avg_PAY']/df['LIMIT_BAL'])\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing variables not needed in the analysis\n",
    ">We will conduct one classification task and one regression task. We will create 2 different objects since we will remove different variables\n",
    "\n",
    "> We first create the object for the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WE CREATE AN OBJECT FOR THE CLASSIFICATION TASK AND OTHER FOR THE REGRESSION TASK\n",
    "\n",
    "# CREATING THE OBJECT FOR THE CLASSIFICATION TASK\n",
    "class_t=df.copy()\n",
    "\n",
    "\n",
    "if 'default_next_m' in class_t:\n",
    "    y_c = class_t['default_next_m'].values\n",
    "    del class_t['default_next_m']\n",
    "    del class_t['ID']\n",
    "    del class_t['EDUCATION']\n",
    "    del class_t['MARRIAGE']\n",
    "    del class_t['PAY_1']\n",
    "    del class_t['PAY_2']\n",
    "    del class_t['PAY_3']\n",
    "    del class_t['PAY_4']\n",
    "    del class_t['PAY_5']\n",
    "    del class_t['PAY_6']\n",
    "    X_c = class_t.values\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then we create the object for the Regression Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0642    , 0.02371806, 0.1882463 , ..., 0.39164444, 0.55543958,\n",
       "       0.76958   ])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### WE CREATE AN OBJECT FOR THE CLASSIFICATION TASK AND OTHER FOR THE REGRESSION TASK\n",
    "\n",
    "# CREATING THE OBJECT FOR THE REGRESSION  TASK\n",
    "\n",
    "reg_t=df.copy()\n",
    "\n",
    "\n",
    "if 'AvgBill_to_LIMIT_BAL' in reg_t:\n",
    "    y_r = reg_t['AvgBill_to_LIMIT_BAL'].values\n",
    "    del reg_t['AvgBill_to_LIMIT_BAL']\n",
    "    del reg_t['ID']\n",
    "    del reg_t['EDUCATION']\n",
    "    del reg_t['MARRIAGE']\n",
    "    del reg_t['PAY_1']\n",
    "    del reg_t['PAY_2']\n",
    "    del reg_t['PAY_3']\n",
    "    del reg_t['PAY_4']\n",
    "    del reg_t['PAY_5']\n",
    "    del reg_t['PAY_6']\n",
    "    del reg_t['Avg_BILL']\n",
    "    del reg_t['LIMIT_BAL']\n",
    "    del reg_t['AvgPay_to_LIMIT_BAL']\n",
    "    del reg_t['BILL_AMT1']\n",
    "    del reg_t['BILL_AMT2']\n",
    "    del reg_t['BILL_AMT3']\n",
    "    del reg_t['BILL_AMT4']\n",
    "    del reg_t['BILL_AMT5']\n",
    "    del reg_t['BILL_AMT6']\n",
    "    X_r = reg_t.values\n",
    "\n",
    "#len(y_r)\n",
    "y_r\n",
    "#reg_t.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Dataset-Description\"></a>Dataset Description\n",
    "\n",
    ">[5 points] Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "\n",
    "# <a name=\"Data-Meaning-Type\"></a>Data Meaning Type\n",
    "*Describe the meaning and type of data (scale, values, etc.) for each attribute in the data file.*\n",
    "\n",
    "> The original dataset contains the following data types:\n",
    "\n",
    "| Variable | Variable Type | Description |\n",
    "| :------: | :-----------: | :----- |\n",
    "| LIMIT_BAL | int64 | The amount of the given credit in New Taiwan dollars.<br/>It includes both the individual consumer credit and his/her family (supplementary) credit |\n",
    "| SEX | int64 | Defines each subjects gender with:<ul><li>1 = Male</li><li>2 = Female</li></ul> |\n",
    "| EDUCATION | int64 | Represents each subject level of education with:<ul><li>1 = Graduate School</li><li>2 = University</li><li>3 = High School</li><li>4 = Other</li></ul> |\n",
    "| MARRIAGE | int64 | Defines the person's relationship status by:<ul><li>1 = Married</li><li>2 = Single</li><li>3 = Other</li></ul> |\n",
    "| AGE | int64 | Defines how old each person is in years |\n",
    "| PAY_0 | int64 | The repayment status in September, 2005 |\n",
    "| PAY_2 | int64 | The repayment status in August, 2005 |\n",
    "| PAY_3 | int64 | The repayment status in July, 2005 |\n",
    "| PAY_4 | int64 | The repayment status in June, 2005 |\n",
    "| PAY_5 | int64 | The repayment status in May, 2005 |\n",
    "| PAY_6 | int64 | The repayment status in April, 2005 |\n",
    "| BILL_AMT1 | int64 | The repayment status in September, 2005 |\n",
    "| BILL_AMT2 | int64 | The repayment status in August, 2005 |\n",
    "| BILL_AMT3 | int64 | The repayment status in July, 2005 |\n",
    "| BILL_AMT4 | int64 | The repayment status in June, 2005 |\n",
    "| BILL_AMT5 | int64 | The repayment status in May, 2005 |\n",
    "| BILL_AMT6 | int64 | The repayment status in April, 2005 |\n",
    "| PAY_AMT1 | int64 | The amount paid in September, 2005 |\n",
    "| PAY_AMT2 | int64 | The amount paid in August, 2005 |\n",
    "| PAY_AMT3 | int64 | The amount paid in July, 2005 |\n",
    "| PAY_AMT4 | int64 | The amount paid in June, 2005 |\n",
    "| PAY_AMT5 | int64 | The amount paid in May, 2005 |\n",
    "| PAY_AMT6 | int64 | The amount paid in April, 2005 |\n",
    "| default payment next month | int64 | A binary value indicating default status with:<ul><li>0 = Customer has defaulted</li><li>1 = Customer has not defaulted</li></ul>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description for the classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lab1 var for class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description for the Regression task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To complete the regression task, several columns were removed. The remaining variables include age, payment amounts, default status for next month, the imputed values of education and marital status, delay in repayment status, sex, and average payment amount. The four new created variables, as mentioned above, were added to the regression dataset as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Evaluation-Metrics-Description\"></a>Evaluation Metrics Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">[10 points] Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
    "precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "roc recall, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Training-Testing\"></a>Training and Testing Splits method\n",
    "\n",
    ">[10 points] Choose the method you will use for dividing your data into training and testing splits \n",
    "(i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is \n",
    "appropriate or use more than one method as appropriate. For example, \n",
    "if you are using time series data then you should be using continuous training and testing sets across time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for classisifations: We chose to use the stratified sampling method becuase our data contains a significantly high number of subjects classified as “0”. Because of this inbalance, there is potential for the algorithm to predict a high number of false negatives and flase  becaue of the class imbalance. The stratified cross validation procedure will ensure the training data sets will have approximately the same amount of subjects who defaulted as those who did not and will subsequently protect the algorithm against false negative predictions\n",
    "\n",
    "for regeression: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING THE CV OBJECT FOR CLASSIFICATION\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_cv_iterations = 10\n",
    "\n",
    "cv_object = StratifiedShuffleSplit(n_splits = num_cv_iterations, \n",
    "                            test_size = 0.20, train_size = 0.80, random_state=1)\n",
    "\n",
    "\n",
    "##### We generate the training and testing sample\n",
    "for train_indices, test_indices in cv_object.split(X_c,y_c): \n",
    "    X_c_train = X_c[train_indices]\n",
    "    y_c_train = y_c[train_indices]\n",
    "    \n",
    "    X_c_test = X_c[test_indices]\n",
    "    y_c_test = y_c[test_indices]\n",
    "\n",
    "\n",
    "#### SCALING THE OBJECTS\n",
    "    \n",
    "\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_c_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_c_train_scaled = scl_obj.transform(X_c_train) # apply to training\n",
    "X_c_test_scaled = scl_obj.transform(X_c_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING THE CV OBJECT FOR REGRESSION\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "#cv = ShuffleSplit(n_splits=10, test_size=0.10, random_state=0)\n",
    "#from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "num_cv_iterations = 10\n",
    "\n",
    "cv_reg = ShuffleSplit(n_splits = num_cv_iterations, \n",
    "                            test_size = 0.20, train_size = 0.80, random_state=1)\n",
    "\n",
    "\n",
    "##### We generate the training and testing sample\n",
    "for train_indices, test_indices in cv_reg.split(X_r,y_r): \n",
    "    X_r_train = X_r[train_indices]\n",
    "    y_r_train = y_r[train_indices]\n",
    "    \n",
    "    X_r_test = X_r[test_indices]\n",
    "    y_r_test = y_r[test_indices]\n",
    "\n",
    "\n",
    "#cv_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Classification-Task\"></a>Classification Task\n",
    "\n",
    "> [20 points] Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09455901  0.84576276  0.02488277  0.03479546]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Source:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                            n_informative=2, n_redundant=0,\n",
    "                            random_state=0, shuffle=False)\n",
    "clf = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "clf.fit(X, y)\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "print(clf.feature_importances_)\n",
    "print(clf.predict([[0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## KNN NEIGHBORS Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Regression-Task\"></a>Regression Task\n",
    "\n",
    ">[20 points] Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Scorers for Evaluating Regression Models \n",
    "\n",
    "**All regression models created in this notebook are validated using the following metrics:**\n",
    "* Mean Absolute Error (MAE)\n",
    "* Root Mean Squared Error (RMSE) - https://stackoverflow.com/questions/17197492/root-mean-square-error-in-python\n",
    "* Mean Absolute Percentage Error (MAPE) - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
    "\n",
    "**For details on making scorers to return multiple mean error scores see:**\n",
    "* http://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html\n",
    "* https://github.com/scikit-learn/scikit-learn/pull/7388\n",
    "* https://github.com/drorata/multiscorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SETTING UP THE PERFORMANCE METRICS\n",
    "#Use mean absolute error (MAE) to score the regression models created \n",
    "#(the scale of MAE is identical to the response variable)\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error\n",
    "\n",
    "#Function for Root mean squared error\n",
    "#https://stackoverflow.com/questions/17197492/root-mean-square-error-in-python\n",
    "def rmse(y_actual, y_predicted):\n",
    "    return np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\n",
    "#Function for Mean Absolute Percentage Error (MAPE) - Untested\n",
    "#Adapted from - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
    "def mape(y_actual, y_predicted): \n",
    "    mask = y_actual != 0\n",
    "    return (np.fabs(y_actual - y_predicted)/y_actual)[mask].mean() * 100\n",
    "\n",
    "#Create scorers for rmse and mape functions\n",
    "mae_scorer = make_scorer(score_func=mean_absolute_error, greater_is_better=False)\n",
    "rmse_scorer = make_scorer(score_func=rmse, greater_is_better=False)\n",
    "mape_scorer = make_scorer(score_func=mape, greater_is_better=False)\n",
    "\n",
    "#Make scorer array to pass into cross_validate() function for producing mutiple scores for each cv fold.\n",
    "errorScoring = {'MAE':  mae_scorer, \n",
    "                'RMSE': rmse_scorer,\n",
    "                'MAPE': mape_scorer\n",
    "               } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model Evaluation\n",
    "** All regression models are evaluated using the regression model evaluation function below: ** \n",
    "* The following regression evaluation function uses the cross validation object and the custom scorers in the two cells above in combination with sklearn.model_selection's cross_validate function to perform cross validation for regression estimators.\n",
    "* The cross validation object above uses a random seed to ensure that all regression estimators are tested on the same randomly selected records for each cross validation fold.\n",
    "* Custom scorers are created using the three chosen mean error scores and passed into cross_validate(), so all three scores are calcualted using a single call to cross_validate().\n",
    "* All of this functionality is wrapped within the custom EvaluateRegressionEstimator() function below so multiple regression models may be tested using the same test / train cv data and evaluation scores producing a consistent output for each model without the need to re-write the same code over and over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def EvaluateRegressionEstimator(regEstimator, X, y, cv):\n",
    "    \n",
    "    scores = cross_validate(regEstimator, X, y, scoring=errorScoring, cv=cv, return_train_score=True)\n",
    "\n",
    "    #cross val score sign-flips the outputs of MAE\n",
    "    # https://github.com/scikit-learn/scikit-learn/issues/2439\n",
    "    scores['test_MAE'] = scores['test_MAE'] * -1\n",
    "    scores['test_MAPE'] = scores['test_MAPE'] * -1\n",
    "    scores['test_RMSE'] = scores['test_RMSE'] * -1\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    maeAvg = scores['test_MAE'].mean()\n",
    "    print_str = \"The average MAE for all cv folds is: \\t\\t\\t {maeAvg:.5}\"\n",
    "    print(print_str.format(maeAvg=maeAvg))\n",
    "\n",
    "    #print mean test_MAPE for all folds\n",
    "    scores['test_MAPE'] = scores['test_MAPE']\n",
    "    mape_avg = scores['test_MAPE'].mean()\n",
    "    print_str = \"The average MAE percentage (MAPE) for all cv folds is: \\t {mape_avg:.5}\"\n",
    "    print(print_str.format(mape_avg=mape_avg))\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    RMSEavg = scores['test_RMSE'].mean()\n",
    "    print_str = \"The average RMSE for all cv folds is: \\t\\t\\t {RMSEavg:.5}\"\n",
    "    print(print_str.format(RMSEavg=RMSEavg))\n",
    "    print('*********************************************************')\n",
    "\n",
    "    print('Cross Validation Fold Mean Error Scores')\n",
    "    scoresResults = pd.DataFrame()\n",
    "    scoresResults['MAE'] = scores['test_MAE']\n",
    "    scoresResults['MAPE'] = scores['test_MAPE']\n",
    "    scoresResults['RMSE'] = scores['test_RMSE']\n",
    "    return scoresResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Regression Model\n",
    "\n",
    "**Linear Regression is used to create a baseline model.  Since linear regression may predict response variable values outside the range of the training data's response variable, we create a linear regression estimator with minority percentage predictions clipped 0% and 100%. For details see:**\n",
    "* http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator \n",
    "* https://github.com/scikit-learn/scikit-learn/issues/6950\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "* https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/template.py\n",
    "* https://stackoverflow.com/questions/44234682/how-to-use-sklearn-when-target-variable-is-a-proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make new estimator compatible for use with GridSearchCV() and cross_validate()\n",
    "# -  Cap predict function for LinearRegression between 0 and 100\n",
    "# -  See: Roll your own estimator links above for details. \n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class CappedLinearRegression(LinearRegression):\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.clip(super(CappedLinearRegression, self).predict(X), 0, 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Grid Search\n",
    "** Here we perform a grid search testing 40 models to find the best parameters for our Linear Regression model based on Mean Absolute Error.  See more on parameter tuning with grid search here:**\n",
    "* http://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=1, test_size=0.2, train_size=0.8),\n",
       "       error_score='raise',\n",
       "       estimator=CappedLinearRegression(copy_X=True, fit_intercept=True, n_jobs=1,\n",
       "            normalize=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'normalize': (True, False), 'fit_intercept': (True, False)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(mean_absolute_error, greater_is_better=False),\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a Linear Regression object and perform a grid search to find the best parameters\n",
    "linreg = CappedLinearRegression()\n",
    "parameters = {'normalize':(True,False), 'fit_intercept':(True,False)}\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=linreg\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv_reg # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "#regGridSearch.fit(X_r, y_r)\n",
    "regGridSearch.fit(X_r_train, y_r_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CappedLinearRegression(copy_X=True, fit_intercept=True, n_jobs=1,\n",
       "            normalize=False)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the parameterization of the best estimator\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Baseline Regression Model - Cross Validation\n",
    "**Perform tenfold cross validation using the grid search \"best\" parameters and our Capped Linear Regression estimator**\n",
    "* 10-fold cross-validation using the parameters for the top performing model \n",
    "* CAP predictions between 0 and 100% \n",
    "* Evaluate cross-validation results using MAE, MAPE, and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average MAE for all cv folds is: \t\t\t 0.27763\n",
      "The average MAE percentage (MAPE) for all cv folds is: \t 625.39\n",
      "The average RMSE for all cv folds is: \t\t\t 0.32532\n",
      "*********************************************************\n",
      "Cross Validation Fold Mean Error Scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.278346</td>\n",
       "      <td>-1537.906701</td>\n",
       "      <td>0.324814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.279350</td>\n",
       "      <td>1921.055181</td>\n",
       "      <td>0.327116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.273167</td>\n",
       "      <td>-1642.538180</td>\n",
       "      <td>0.318002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.277416</td>\n",
       "      <td>-1493.481073</td>\n",
       "      <td>0.327031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276687</td>\n",
       "      <td>1858.570440</td>\n",
       "      <td>0.321749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.279477</td>\n",
       "      <td>1966.131017</td>\n",
       "      <td>0.329058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.275913</td>\n",
       "      <td>1655.264369</td>\n",
       "      <td>0.326384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.280647</td>\n",
       "      <td>1430.388675</td>\n",
       "      <td>0.329018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.278445</td>\n",
       "      <td>912.709904</td>\n",
       "      <td>0.325872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.276810</td>\n",
       "      <td>1183.682340</td>\n",
       "      <td>0.324132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MAE         MAPE      RMSE\n",
       "0  0.278346 -1537.906701  0.324814\n",
       "1  0.279350  1921.055181  0.327116\n",
       "2  0.273167 -1642.538180  0.318002\n",
       "3  0.277416 -1493.481073  0.327031\n",
       "4  0.276687  1858.570440  0.321749\n",
       "5  0.279477  1966.131017  0.329058\n",
       "6  0.275913  1655.264369  0.326384\n",
       "7  0.280647  1430.388675  0.329018\n",
       "8  0.278445   912.709904  0.325872\n",
       "9  0.276810  1183.682340  0.324132"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create CappedLinearRegression predictions between 0 and 100% using the best parameters for our Linear Regression object\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "#EvaluateRegressionEstimator(regEstimator, X_r, y_r, cv_reg)\n",
    "EvaluateRegressionEstimator(regEstimator, X_r_train, y_r_train, cv_reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lasso Regression\n",
    "**Perform regression using Linear Model trained with L1 prior as regularizer (aka the Lasso)**\n",
    "\n",
    "Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    9.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=1, test_size=0.2, train_size=0.8),\n",
       "       error_score='raise',\n",
       "       estimator=Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=10000,\n",
       "   normalize=True, positive=False, precompute=True, random_state=0,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=8,\n",
       "       param_grid={'alpha': [0.001, 0.1, 1, 10, 20], 'warm_start': [True, False], 'selection': ['cyclic', 'random']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(mean_absolute_error, greater_is_better=False),\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a regression object and perform a grid search to find the best parameters\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lassoreg = Lasso(fit_intercept=True, normalize=True,copy_X=True\n",
    "          , max_iter=10000, precompute=True, tol=0.0001, random_state=0)\n",
    "\n",
    "#Test parameters \n",
    "alpha = [0.001, 0.1, 1, 10, 20]\n",
    "selection = ['cyclic','random']\n",
    "warm_start = [True, False]\n",
    "parameters = {'alpha': alpha, 'selection': selection, 'warm_start': warm_start}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=lassoreg\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv_reg # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X_r, y_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=10000,\n",
       "   normalize=True, positive=False, precompute=True, random_state=0,\n",
       "   selection='cyclic', tol=0.0001, warm_start=True)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the best estimator parameters\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average MAE for all cv folds is: \t\t\t 0.31171\n",
      "The average MAE percentage (MAPE) for all cv folds is: \t -379.45\n",
      "The average RMSE for all cv folds is: \t\t\t 0.353\n",
      "*********************************************************\n",
      "Cross Validation Fold Mean Error Scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.312945</td>\n",
       "      <td>1968.562203</td>\n",
       "      <td>0.357046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.310933</td>\n",
       "      <td>-557.090559</td>\n",
       "      <td>0.349272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.310459</td>\n",
       "      <td>-2133.992640</td>\n",
       "      <td>0.350454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.309206</td>\n",
       "      <td>821.386470</td>\n",
       "      <td>0.346295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.312353</td>\n",
       "      <td>-848.455682</td>\n",
       "      <td>0.352686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.307881</td>\n",
       "      <td>-2452.067162</td>\n",
       "      <td>0.348565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.313857</td>\n",
       "      <td>-1264.995682</td>\n",
       "      <td>0.355773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.315162</td>\n",
       "      <td>548.224661</td>\n",
       "      <td>0.361312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.311389</td>\n",
       "      <td>-1114.752323</td>\n",
       "      <td>0.350986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.312897</td>\n",
       "      <td>1238.684465</td>\n",
       "      <td>0.357586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MAE         MAPE      RMSE\n",
       "0  0.312945  1968.562203  0.357046\n",
       "1  0.310933  -557.090559  0.349272\n",
       "2  0.310459 -2133.992640  0.350454\n",
       "3  0.309206   821.386470  0.346295\n",
       "4  0.312353  -848.455682  0.352686\n",
       "5  0.307881 -2452.067162  0.348565\n",
       "6  0.313857 -1264.995682  0.355773\n",
       "7  0.315162   548.224661  0.361312\n",
       "8  0.311389 -1114.752323  0.350986\n",
       "9  0.312897  1238.684465  0.357586"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a regression estimator with best parameters for cross validation\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "EvaluateRegressionEstimator(regEstimator, X_r, y_r, cv_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yhat Max:  0.373047945753\n"
     ]
    }
   ],
   "source": [
    "#Do we predict Average Bill amount to card Limit Balance ratios greater than 100%?\n",
    "regEstimator = Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=10000,\n",
    "   normalize=True, positive=False, precompute=True, random_state=0,\n",
    "   selection='cyclic', tol=0.0001, warm_start=True)\n",
    "\n",
    "regEstimator.fit(X_r, y_r)\n",
    "yhat = regEstimator.predict(X_r)\n",
    "print(\"Yhat Max: \", yhat.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Ridge Regression\n",
    "**Perform regression using Linear least squares with l2 regularization**\n",
    "\n",
    "Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=8)]: Done 360 out of 360 | elapsed:   30.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=1, test_size=0.2, train_size=0.8),\n",
       "       error_score='raise',\n",
       "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=True, random_state=0, solver='auto', tol=0.0001),\n",
       "       fit_params=None, iid=True, n_jobs=8,\n",
       "       param_grid={'solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'], 'alpha': [0.001, 0.1, 1, 5, 10, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(mean_absolute_error, greater_is_better=False),\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a regression object and perform a grid search to find the best parameters\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridgereg = Ridge(fit_intercept=True, normalize=True,copy_X=True\n",
    "          , max_iter=1000, tol=0.0001, random_state=0)\n",
    "\n",
    "#Test parameters \n",
    "alpha = [0.001, 0.1, 1, 5, 10, 20]\n",
    "solver = [ 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "parameters = {'alpha': alpha, 'solver': solver}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=ridgereg\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv_reg # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X_r, y_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=True, random_state=0, solver='lsqr', tol=0.0001)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the best estimator parameters\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average MAE for all cv folds is: \t\t\t 0.27825\n",
      "The average MAE percentage (MAPE) for all cv folds is: \t 351.59\n",
      "The average RMSE for all cv folds is: \t\t\t 0.32815\n",
      "*********************************************************\n",
      "Cross Validation Fold Mean Error Scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.280175</td>\n",
       "      <td>1662.023074</td>\n",
       "      <td>0.332624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.276132</td>\n",
       "      <td>-539.584402</td>\n",
       "      <td>0.321971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.278355</td>\n",
       "      <td>-1255.239839</td>\n",
       "      <td>0.326401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.276066</td>\n",
       "      <td>1338.978267</td>\n",
       "      <td>0.322782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.277515</td>\n",
       "      <td>810.626879</td>\n",
       "      <td>0.324810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.275993</td>\n",
       "      <td>-233.839916</td>\n",
       "      <td>0.324786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.280331</td>\n",
       "      <td>-612.434324</td>\n",
       "      <td>0.330710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.279359</td>\n",
       "      <td>442.836824</td>\n",
       "      <td>0.336359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.278508</td>\n",
       "      <td>641.759902</td>\n",
       "      <td>0.326381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.280114</td>\n",
       "      <td>1260.757413</td>\n",
       "      <td>0.334655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MAE         MAPE      RMSE\n",
       "0  0.280175  1662.023074  0.332624\n",
       "1  0.276132  -539.584402  0.321971\n",
       "2  0.278355 -1255.239839  0.326401\n",
       "3  0.276066  1338.978267  0.322782\n",
       "4  0.277515   810.626879  0.324810\n",
       "5  0.275993  -233.839916  0.324786\n",
       "6  0.280331  -612.434324  0.330710\n",
       "7  0.279359   442.836824  0.336359\n",
       "8  0.278508   641.759902  0.326381\n",
       "9  0.280114  1260.757413  0.334655"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a regression estimator with best parameters for cross validation\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "EvaluateRegressionEstimator(regEstimator, X_r, y_r, cv_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yhat Max:  2.21058516598\n"
     ]
    }
   ],
   "source": [
    "#Do we predict Average Bill amount to card Limit Balance ratios greater than 100%?\n",
    "regEstimator = Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
    "   normalize=True, random_state=0, solver='saga', tol=0.0001)\n",
    "\n",
    "regEstimator.fit(X_r, y_r)\n",
    "yhat = regEstimator.predict(X_r)\n",
    "print(\"Yhat Max: \", yhat.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Methods-of-Evaluation\"></a>Methods of Evaluation Results\n",
    "\n",
    "> [10 points] Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification: ROC, acc, recall. Regression: mse, acc, rmse, mae mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Advantages-of-each-model\"></a>Advantages of each model\n",
    ">[10 points] Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the cours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all add your own part. we can clean up after all ideas are put down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Relevant-Attributes\"></a>Relevant Attributes\n",
    ">[10 points] Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all add your own part. we can clean up after all ideas are put down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Deployment\"></a>Deployment\n",
    ">[5 points] How useful is your model for interested parties (i.e., the companies or organizations\n",
    "that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all add your own part. we can clean up after all ideas are put down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Exceptional-Work\"></a>Exceptional Work\n",
    "> [10 points] You have free reign to provide additional analyses\n",
    "> One idea: grid search parameters in a parallelized fashion and visualize the\n",
    "performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid search- ernesto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"Training-and-Testing-Split\"></a>Training and Testing Split\n",
    "\n",
    "## Stratified K-Folds Cross-Validation\n",
    "\n",
    "> There is a very apparent class imbalance within the dataset. In order to address this issue, stratified K-Folds cross-validation will be employed to preserve the percentage of samples for each class. In other words, this will prevent the model from simply guessing 'paid duly' for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = credit\n",
    "#EC: Creating  another reference in MEMORY, so the changes in df will not be REFLECTED IN CREDIT\n",
    "#df = credit.copy()\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "if 'default_next_m' in df:\n",
    "    y_K = df['default_next_m'].values\n",
    "    del df['default_next_m']\n",
    "    del df['ID']\n",
    "    del df['PAY_1']\n",
    "    del df['PAY_2']\n",
    "    del df['PAY_3']\n",
    "    del df['PAY_4']\n",
    "    del df['PAY_5']\n",
    "    del df['PAY_6']\n",
    "    del df['Avg_BILL']\n",
    "    del df['LIMIT_BAL']\n",
    "    del df['AvgPay_to_LIMIT_BAL']\n",
    "    del df['BILL_AMT1']\n",
    "    del df['BILL_AMT2']\n",
    "    del df['BILL_AMT3']\n",
    "    del df['BILL_AMT4']\n",
    "    del df['BILL_AMT5']\n",
    "    del df['BILL_AMT6']\n",
    "    X_K = df.values\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = StratifiedShuffleSplit(n_splits = num_cv_iterations, \n",
    "                            test_size = 0.20, train_size = 0.80, random_state=1)\n",
    "cv_object.get_n_splits(X, y)\n",
    "print(cv_object)\n",
    "for train_index, test_index in cv_object.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('# of obs in train set: ' + str(len(y_train)))\n",
    "print('# of obs in test set: ' + str(len(y_test)))\n",
    "\n",
    "#train_defaults = sum(y_train == 0)\n",
    "train_defaults = sum(y_train == 1)\n",
    "print('# of defaults in train set: ' + str(train_defaults))\n",
    "\n",
    "proportion_train_defaults = train_defaults / len(y_train)\n",
    "print('Proportion of defaults in train set: ' + str(proportion_train_defaults))\n",
    "\n",
    "#test_defaults = sum(y_test == 0)\n",
    "test_defaults = sum(y_test == 1)\n",
    "print('# of defaults in test set: ' + str(test_defaults))\n",
    "\n",
    "proportion_test_defaults = test_defaults / len(y_test)\n",
    "print('Proportion of defaults in test set: ' + str(proportion_test_defaults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('# of obs in train set: ' + str(len(y_train)))\n",
    "print('# of obs in test set: ' + str(len(y_test)))\n",
    "\n",
    "#train_defaults = sum(y_train == 0)\n",
    "train_defaults = sum(y_train == 1)\n",
    "print('# of defaults in train set: ' + str(train_defaults))\n",
    "\n",
    "proportion_train_defaults = train_defaults / len(y_train)\n",
    "print('Proportion of defaults in train set: ' + str(proportion_train_defaults))\n",
    "\n",
    "#test_defaults = sum(y_test == 0)\n",
    "test_defaults = sum(y_test == 1)\n",
    "print('# of defaults in test set: ' + str(test_defaults))\n",
    "\n",
    "proportion_test_defaults = test_defaults / len(y_test)\n",
    "print('Proportion of defaults in test set: ' + str(proportion_test_defaults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.html import widgets \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn import metrics as mt\n",
    "\n",
    "\n",
    "max_accs = 0\n",
    "max_k = 0\n",
    "\n",
    "for k in range(5,15):\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, weights='uniform', metric='cosine')\n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat = clf.predict(X_test)\n",
    "    acc = mt.accuracy_score(y_test,yhat)\n",
    "    if max(max_accs, acc) == acc:\n",
    "        max_accs = acc\n",
    "        max_k = k\n",
    "    print('Accuracy of', k, 'neighbors is', acc * 100, '%')\n",
    "\n",
    "print('\\nThe best accuracy is %f with %d neighbor(s)'%(max_accs,max_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "\n",
    "for d in ['l1','l2','cosine']:\n",
    "    clf = NearestCentroid(metric=d)\n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat = clf.predict(X_test)\n",
    "    acc = mt.accuracy_score(y_test,yhat)\n",
    "    print('\\nAccuracy of', d, 'metrics is', acc * 100, '%')    \n",
    "    print('The best distance metric is: ', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(penalty='l2', C=1, class_weight='balanced') # get object\n",
    "#WARNING: THE FIRST WIEGHT WAS 1 \n",
    "iter_num=0\n",
    "times_rec=[]\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "     #we count the time in executing the logistic regression\n",
    "    t0 = time()\n",
    "    \n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "    \n",
    "    t1=time()\n",
    "    diff=np.array([t1-t0])\n",
    "    \n",
    "    print (\"The time it takes to fit and predict is \" + str(diff[0]) + \"\\n\")    \n",
    "    times_rec=np.append(times_rec,diff)\n",
    "    \n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    \n",
    "    print(\"\\n *** CLASSIFICATION REPORT ****\")\n",
    "    #### CLASSIFICATION REPORT\n",
    "    ClassReport = mt.classification_report(y_test,y_hat)\n",
    "    print(ClassReport)\n",
    "    \n",
    "    iter_num+=1\n",
    "    \n",
    "    \n",
    "print(\"The average time to fit and predict 3 logistic regressions with 80/20 training/test split is: \" + str(times_rec.mean()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <a name=\"References\"></a>References\n",
    "\n",
    "* https://github.com/eclarson/DataMiningNotebooks/blob/master/04.%20Logits%20and%20SVM.ipynb\n",
    "* https://github.com/jakemdrew/EducationDataNC/blob/master/Graduation%20Rates%20v2.ipynb\n",
    "* http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "* https://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn\n",
    "* http://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/\n",
    "* https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
